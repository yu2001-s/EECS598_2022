{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e2e14c4",
   "metadata": {
    "id": "0e2e14c4"
   },
   "source": [
    "# EECS 498-007/598-005 Assignment 5-2: Transformers\n",
    "\n",
    "Before we start, please put your name and UMID in following format\n",
    "\n",
    ": Firstname LASTNAME, #00000000   //   e.g.) Justin JOHNSON, #12345678"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7719a8a0",
   "metadata": {
    "id": "7719a8a0"
   },
   "source": [
    "**Your Answer:**\\\n",
    "Hello WORLD, #XXXXXXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819e7379",
   "metadata": {
    "id": "819e7379"
   },
   "source": [
    "### Transformers ([Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf))\n",
    "\n",
    "To this point we have seen RNNs, which excel at sequence to sequence task but have two major drawbacks.\n",
    "First, they can suffer from vanishing gradients for long sequences.\n",
    "Second, they can take a long time to train due to sequential dependencies between hidden states which does not take advantage of the massively parallel architecture of modern GPUs.\n",
    "The first issue is largely addressed by alternate RNN architectures (LSTMs, GRUs) but not the second.\n",
    "\n",
    "Transformers solve these problems up to a certain extent by enabling to process the input parallely during training with long sequences. Though the computation is quadratic with respect to the input sequence length, it still managable with modern GPUs.\n",
    "\n",
    "In this notebook, we will implement Transformers model step-by-step by referencing the original paper, [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf). We will also use a toy dataset to solve a vector-to-vector problem which is a subset of sequence-to-sequence problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8848024",
   "metadata": {
    "id": "e8848024"
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "This assignment has 4 parts. In the class we learned about Encoder based Transformers but often we use an Encoder and a Decoder for sequence to sequence task. In this notebook, you will learn how to implement an Encoder-Decoder based Transformers in a step-by-step manner. We will implement a simpler version here, where the simplicity arise from the task that we are solving, which is a vector-to-vector task. This essentially means that the length of input and output sequence is **fixed** and we dont have to worry about variable length of sequences. This makes the implementation simpler.\n",
    "\n",
    "1. **Part I (Preparation)**: We will preprocess a toy dataset that consists of input arithmetic expression and an output result of the expression\n",
    "1. **Part II (Implement Transformer blocks)**: we will look how to implement building blocks of a Transformer. It will consist of following blocks\n",
    "   1. MultiHeadAttention\n",
    "   2. FeedForward\n",
    "   3. LayerNorm\n",
    "   4. Encoder Block\n",
    "   5. Decoder Block\n",
    "1. **Part III (Data Loading)**: We will use the preprocessing functions in part I and the positional encoding module to construct the Dataloader.\n",
    "1. **Part IV (Train a model)**: In the last part we will look at how to fit the implemented Transformer model to the toy dataset.\n",
    "\n",
    "You can run all things on CPU till part 3. Part 4 requires GPU and while changing the runtime for this part, you would also have to run all the previous parts as part 4 has dependency on previous parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1c7486",
   "metadata": {
    "id": "3e1c7486"
   },
   "source": [
    "# Part I. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdbbed0",
   "metadata": {
    "id": "0bdbbed0"
   },
   "source": [
    "Before getting started we need to run some boilerplate code to set up our environment. You\"ll need to rerun this setup code each time you start the notebook.\n",
    "\n",
    "First, run this cell load the [autoreload](https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html?highlight=autoreload) extension. This allows us to edit `.py` source files, and re-import them into the notebook for a seamless editing and debugging experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7dede27",
   "metadata": {
    "id": "f7dede27"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1523c0",
   "metadata": {
    "id": "6c1523c0"
   },
   "source": [
    "### Google Colab Setup\n",
    "\n",
    "Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n",
    "\n",
    "Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff4169c8",
   "metadata": {
    "id": "ff4169c8"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6979e52e",
   "metadata": {
    "id": "6979e52e"
   },
   "source": [
    "Now recall the path in your Google Drive where you uploaded this notebook, fill it in below. If everything is working correctly then running the folowing cell should print the filenames from the assignment:\n",
    "\n",
    "```\n",
    "[\"eecs598\", \"a5_helper.py\", \"rnn_lstm_attention_captioning.ipynb\",  \"rnn_lstm_attention_captioning.py\", \"Transformers.py\", \"Transformers.ipynb\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e1beb7a",
   "metadata": {
    "id": "1e1beb7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a5_helper.py', 'eecs598', 'Transformers.ipynb', 'transformer.pt', '__pycache__', 'transformers.py', 'two_digit_op.json', 'rnn_lstm_captioning.ipynb', 'rnn_lstm_captioning.py', 'datasets']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# TODO: Fill in the Google Drive path where you uploaded the assignment\n",
    "# Example: If you create a WI2022 folder and put all the files under A5 folder, then \"WI2022/A5\"\n",
    "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = None\n",
    "\n",
    "GOOGLE_DRIVE_PATH = os.getcwd()\n",
    "print(os.listdir(GOOGLE_DRIVE_PATH))\n",
    "\n",
    "\n",
    "# Add to sys so we can import .py files.\n",
    "\n",
    "sys.path.append(GOOGLE_DRIVE_PATH) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ea3c66",
   "metadata": {
    "id": "c0ea3c66"
   },
   "source": [
    "Once you have successfully mounted your Google Drive and located the path to this assignment, run th following cell to allow us to import from the `.py` files of this assignment. If it works correctly, it should print the message:\n",
    "\n",
    "```\n",
    "Hello from Transformers.py!\n",
    "```\n",
    "\n",
    "as well as the last edit time for the file `Transformers.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac76ec4e",
   "metadata": {
    "id": "ac76ec4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from transformers.py!\n",
      "transformers.py last edited on Thu Nov 16 05:39:25 2023\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from transformers import hello_transformers\n",
    "\n",
    "\n",
    "os.environ[\"TZ\"] = \"US/Eastern\"\n",
    "time.tzset()\n",
    "hello_transformers()\n",
    "\n",
    "transformers_path = os.path.join(GOOGLE_DRIVE_PATH, \"transformers.py\")\n",
    "transformers_edit_time = time.ctime(os.path.getmtime(transformers_path))\n",
    "print(\"transformers.py last edited on %s\" % transformers_edit_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cbf5f86",
   "metadata": {
    "id": "2cbf5f86"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from eecs598.utils import (\n",
    "    reset_seed,\n",
    "    tensor_to_image,\n",
    "    attention_visualizer,\n",
    ")\n",
    "from eecs598.grad import rel_error, compute_numeric_gradient\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "# for plotting\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # set default size of plots\n",
    "plt.rcParams[\"image.interpolation\"] = \"nearest\"\n",
    "plt.rcParams[\"image.cmap\"] = \"gray\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b807888a",
   "metadata": {
    "id": "b807888a"
   },
   "source": [
    "We will use the GPU to accelerate our computation. Run this cell to make sure you are using a GPU.\n",
    "\n",
    "We will be using `torch.float = torch.float32` for data and `torch.long = torch.int64` for labels.\n",
    "\n",
    "Please refer to https://pytorch.org/docs/stable/tensor_attributes.html#torch-dtype for more details about data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0150e9a5",
   "metadata": {
    "id": "0150e9a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good to go!\n"
     ]
    }
   ],
   "source": [
    "to_float = torch.float\n",
    "to_long = torch.long\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Good to go!\")\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Please set GPU via Edit -> Notebook Settings.\")\n",
    "    DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2bcf33",
   "metadata": {
    "id": "5c2bcf33"
   },
   "source": [
    "### Load the toy data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bf6435",
   "metadata": {
    "id": "16bf6435"
   },
   "source": [
    "As Transformers perform very well on sequence to sequence task, we will implement it on a toy task of Arithmetic operations. We will use transformer models to perform addition and subraction of two integers, where the absolute value of an integer is at most 50. A simple example is to perform the computation `-5 + 2` using a Transformer model and getting the corect result as `-3`. As there can be multiple ways to solve this problem, we will see how we can pose this as a sequence to sequence problem and solve it using Transformers model. Note that we had to reduce the complexity of the problem to make the Transformer work within the constrainted resources of Colab.\n",
    "\n",
    "Lets take a look at the data first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3634d982",
   "metadata": {
    "id": "3634d982"
   },
   "outputs": [],
   "source": [
    "from a5_helper import get_toy_data\n",
    "\n",
    "# load the data using helper function\n",
    "data = get_toy_data(os.path.join(GOOGLE_DRIVE_PATH,\"two_digit_op.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0c8b22",
   "metadata": {
    "id": "3f0c8b22"
   },
   "source": [
    "### Looking at the first four examples\n",
    "\n",
    "Below are the first four samples in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0f9a602",
   "metadata": {
    "id": "d0f9a602"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expression: BOS NEGATIVE 30 subtract NEGATIVE 34 EOS Output: BOS POSITIVE 04 EOS\n",
      "Expression: BOS NEGATIVE 34 add NEGATIVE 15 EOS Output: BOS NEGATIVE 49 EOS\n",
      "Expression: BOS NEGATIVE 28 add NEGATIVE 36 EOS Output: BOS NEGATIVE 64 EOS\n",
      "Expression: BOS POSITIVE 00 subtract POSITIVE 17 EOS Output: BOS NEGATIVE 17 EOS\n"
     ]
    }
   ],
   "source": [
    "num_examples = 4\n",
    "for q, a in zip(\n",
    "    data[\"inp_expression\"][:num_examples], \n",
    "    data[\"out_expression\"][:num_examples]\n",
    "    ):\n",
    "  print(\"Expression: \" + q + \" Output: \" + a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc93f275",
   "metadata": {
    "id": "bc93f275"
   },
   "source": [
    "## What do these examples mean:\n",
    "\n",
    "Lets look at first and third examples here and understand what they represent:\n",
    "\n",
    "- Expression: `BOS NEGATIVE 30 subtract NEGATIVE 34 EOS` Output: `BOS POSITIVE 04 EOS`: The expression here is $(-30) - (-34)$. There are two notions of the symbol `+` here: one is to denote the sign of the number and other is the operation of addition between two integers. To simplify the problem for the neural network, we have denoted them with different text tokens. The ouput of $(-30) - (-34)$ is $+4$. Here `BOS` and `EOS` refer to begining of sequence and end of sequence\n",
    "- Similarly, the second expression, `BOS NEGATIVE 34 add NEGATIVE 15 EOS` Output: `BOS NEGATIVE 49 EOS` means that we are doing the computation as $(-34) + (-15)$. As above, the symbol `-` here represents two things: first is the sign of an integer and second is the operation between two integers. Again, we have represented with different tokens to simplify the problem for the neural network. The output here is -49. Here `BOS` and `EOS` refer to begining of sequence and end of sequence\n",
    "\n",
    "Now that we have a grasp on what is the data, lets head to preprocess the data, as the neural networks don't really understand strings, we need to represent them as numbers.\n",
    "\n",
    "## Pre-processing the data\n",
    "We need to convert the raw input sequence into a format that can be processed with a neural network.\n",
    "Concretely, we need to convert a human-readable string (e.g. `BOS NEGATIVE 30 subtract NEGATIVE 34 EOS`) into a sequence of **tokens**, each of which will be an integer.\n",
    "The process of converting an input string into a sequence of tokens is known as **tokenization**.\n",
    "\n",
    "Before we can tokenize any particular sequence, we first need to build a **vocabulary**;\n",
    "this is an exhaustive list of all tokens that appear in our dataset, and a mapping from each token to a unique integer value.\n",
    "In our case, our vocabulary with consist of 16 elements: one entry for each digit `0` to `9`, two tokens to represent the sign of a number (`POSITIVE` and `NEGATIVE`), two tokens representing the addition and subtraction operations (`add`, and `subtract`), and finally two special tokens representing the start and end of the sequence (`BOS`, `EOS`).\n",
    "\n",
    "We typically represent the vocabulary with a pair of data structures.\n",
    "First is a list of all the string tokens (`vocab` below), such that `vocab[i] = s` means that the string `s` has been assigned the integer value `i`. This allows us to look up the string associated with any numeric index `i`.\n",
    "We also need a data structure that enables us to map in the other direction: given a string `s`, find the index `i` to which it has been assigned. This is typically represented as a hash map (`dict` object in Python) whose keys are strings and whose values are the indices assigned to those strings.\n",
    "You will implement the function `generate_token_dict` that inputs the list `vocab` and returns a dict `convert_str_to_token` giving this mapping.\n",
    "\n",
    "Once you have built the vocab, then you can implement the function `preprocess_input_sequence` which uses the vocab data structures to convert an input string into a list of integer tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e87a7b85",
   "metadata": {
    "id": "e87a7b85"
   },
   "outputs": [],
   "source": [
    "# Create vocab\n",
    "SPECIAL_TOKENS = [\"POSITIVE\", \"NEGATIVE\", \"add\", \"subtract\", \"BOS\", \"EOS\"]\n",
    "vocab = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"] + SPECIAL_TOKENS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066cd35e",
   "metadata": {
    "id": "066cd35e"
   },
   "source": [
    "To generate the hash map and then process the input string using them, complete the `generate_token_dict`, `prepocess_input_sequence` functions in the python files for this exercise:\n",
    "\n",
    "You should see exact zero errors here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7beaeebe-76ac-4e59-a244-250c5a18637b",
   "metadata": {
    "id": "7beaeebe-76ac-4e59-a244-250c5a18637b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary created successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import generate_token_dict\n",
    "\n",
    "convert_str_to_tokens = generate_token_dict(vocab)\n",
    "\n",
    "try:\n",
    "    assert convert_str_to_tokens[\"0\"] == 0\n",
    "except:\n",
    "    print(\"The first element does not map to 0. Please check the implementation\")\n",
    "\n",
    "try:\n",
    "    assert convert_str_to_tokens[\"EOS\"] == 15\n",
    "except:\n",
    "    print(\"The last element does not map to 2004. Please check the implementation\")\n",
    "\n",
    "print(\"Dictionary created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b6e4fca",
   "metadata": {
    "id": "5b6e4fca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess input token error 1:  0.0\n",
      "preprocess input token error 2:  0.0\n",
      "preprocess input token error 3:  0.0\n",
      "preprocess input token error 4:  0.0\n",
      "\n",
      "\n",
      "preprocess output token error 1:  0.0\n",
      "preprocess output token error 2:  0.0\n",
      "preprocess output token error 3:  0.0\n",
      "preprocess output token error 4:  0.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import prepocess_input_sequence\n",
    "\n",
    "convert_str_to_tokens = generate_token_dict(vocab)\n",
    "\n",
    "ex1_in = \"BOS POSITIVE 0333 add POSITIVE 0696 EOS\"\n",
    "ex2_in = \"BOS POSITIVE 0673 add POSITIVE 0675 EOS\"\n",
    "ex3_in = \"BOS NEGATIVE 0286 subtract NEGATIVE 0044 EOS\"\n",
    "ex4_in = \"BOS NEGATIVE 0420 add POSITIVE 0342 EOS\"\n",
    "\n",
    "ex1_out = \"BOS POSITIVE 1029 EOS\"\n",
    "ex2_out = \"BOS POSITIVE 1348 EOS\"\n",
    "ex3_out = \"BOS NEGATIVE 0242 EOS\"\n",
    "ex4_out = \"BOS NEGATIVE 0078 EOS\"\n",
    "\n",
    "ex1_inp_preprocessed = torch.tensor(\n",
    "    prepocess_input_sequence(ex1_in, convert_str_to_tokens, SPECIAL_TOKENS)\n",
    ")\n",
    "ex2_inp_preprocessed = torch.tensor(\n",
    "    prepocess_input_sequence(ex2_in, convert_str_to_tokens, SPECIAL_TOKENS)\n",
    ")\n",
    "ex3_inp_preprocessed = torch.tensor(\n",
    "    prepocess_input_sequence(ex3_in, convert_str_to_tokens, SPECIAL_TOKENS)\n",
    ")\n",
    "ex4_inp_preprocessed = torch.tensor(\n",
    "    prepocess_input_sequence(ex4_in, convert_str_to_tokens, SPECIAL_TOKENS)\n",
    ")\n",
    "\n",
    "ex1_processed_expected = torch.tensor([14, 10, 0, 3, 3, 3, 12, 10, 0, 6, 9, 6, 15])\n",
    "ex2_processed_expected = torch.tensor([14, 10, 0, 6, 7, 3, 12, 10, 0, 6, 7, 5, 15])\n",
    "ex3_processed_expected = torch.tensor([14, 11, 0, 2, 8, 6, 13, 11, 0, 0, 4, 4, 15])\n",
    "ex4_processed_expected = torch.tensor([14, 11, 0, 4, 2, 0, 12, 10, 0, 3, 4, 2, 15])\n",
    "\n",
    "ex1_out = torch.tensor(\n",
    "    prepocess_input_sequence(ex1_out, convert_str_to_tokens, SPECIAL_TOKENS)\n",
    ")\n",
    "ex2_out = torch.tensor(\n",
    "    prepocess_input_sequence(ex2_out, convert_str_to_tokens, SPECIAL_TOKENS)\n",
    ")\n",
    "ex3_out = torch.tensor(\n",
    "    prepocess_input_sequence(ex3_out, convert_str_to_tokens, SPECIAL_TOKENS)\n",
    ")\n",
    "ex4_out = torch.tensor(\n",
    "    prepocess_input_sequence(ex4_out, convert_str_to_tokens, SPECIAL_TOKENS)\n",
    ")\n",
    "\n",
    "ex1_out_expected = torch.tensor([14, 10, 1, 0, 2, 9, 15])\n",
    "ex2_out_expected = torch.tensor([14, 10, 1, 3, 4, 8, 15])\n",
    "ex3_out_expected = torch.tensor([14, 11, 0, 2, 4, 2, 15])\n",
    "ex4_out_expected = torch.tensor([14, 11, 0, 0, 7, 8, 15])\n",
    "\n",
    "print(\n",
    "    \"preprocess input token error 1: \",\n",
    "    rel_error(ex1_processed_expected, ex1_inp_preprocessed),\n",
    ")\n",
    "print(\n",
    "    \"preprocess input token error 2: \",\n",
    "    rel_error(ex2_processed_expected, ex2_inp_preprocessed),\n",
    ")\n",
    "print(\n",
    "    \"preprocess input token error 3: \",\n",
    "    rel_error(ex3_processed_expected, ex3_inp_preprocessed),\n",
    ")\n",
    "print(\n",
    "    \"preprocess input token error 4: \",\n",
    "    rel_error(ex4_processed_expected, ex4_inp_preprocessed),\n",
    ")\n",
    "print(\"\\n\")\n",
    "print(\"preprocess output token error 1: \", rel_error(ex1_out_expected, ex1_out))\n",
    "print(\"preprocess output token error 2: \", rel_error(ex2_out_expected, ex2_out))\n",
    "print(\"preprocess output token error 3: \", rel_error(ex3_out_expected, ex3_out))\n",
    "print(\"preprocess output token error 4: \", rel_error(ex4_out_expected, ex4_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aed0ae",
   "metadata": {
    "id": "58aed0ae"
   },
   "source": [
    "# Part II.  Implementing Transformer building blocks\n",
    "\n",
    "Now that we have looked at the data, the task is to predict the output sequence (final result), something like `NEGATIVE 42` given the input sequence (of the arthmetic expression), something like `NEGATIVE 48 subtract NEGATIVE 6`.\n",
    "\n",
    "In this section, we will look at implementing various building blocks used for implementing Transformer model. This will then be used to make Transformer encoder and decoder, which will ultimately lead us to implementing the complete Transfromer model.\n",
    "Each block will be implemented as a subclass of `nn.Module`; we will use PyTorch autograd to compute gradients, so we don't need to implement backward passes manually.\n",
    "\n",
    "We will implement the following blocks, by referencing the original paper:\n",
    "\n",
    "1. MultHeadAttention Block\n",
    "2. FeedForward Block\n",
    "3. Layer Normalization\n",
    "4. Positional Encoding block\n",
    "\n",
    "We will then use these building blocks, combined with the input embedding layer to construct the Transformer Encoder and Decoder. We will start with MultiHeadAttention block, FeedForward Block, and Layer Normalization and look at Position encoding and input embedding later.\n",
    "\n",
    "**Note:** One thing to keep in mind while implementing these blocks is that the shape of input and output Tensor from all these blocks we will be same. It always helps by checking the shapes of inputp and output tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823cc3b9-d483-4d42-a495-eb432d1417e1",
   "metadata": {
    "id": "823cc3b9-d483-4d42-a495-eb432d1417e1"
   },
   "source": [
    "### MultiHeadAttention Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34036573-905d-44f0-8094-71539563ae24",
   "metadata": {
    "id": "34036573-905d-44f0-8094-71539563ae24"
   },
   "source": [
    "The image below highlights the MultiHead Attention block inside the Transformer model.\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1DwU3BJsA0mUWTWlXNtNolB4oc5K4Z9PE\" alt=\"multihead_attention\" width=\"80%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c56e134",
   "metadata": {
    "id": "5c56e134"
   },
   "source": [
    "Transformers are sequence to sequence networks i.e., we get a sequence (for example a sentence in English) and get output a sequence (for example a sentence in Spanish). The input sequence are first transformed into embeddings as discussed in the RNN section and these embeddings are then passed through a Positional Encoding block. The resultant Embeddings are then transformed into three vectors, *query*, *key*, and *value* using learnable weights and we then use a Transformer Encoder and Decoder to get the final output sequence. For this section, we will assume that we have the *query*, *key*, and the *value* vector and work on them.\n",
    "\n",
    "In the above figure, you can see that the Encoder has multihead attention block is right after these blocks. There is also a masked multihead attention in the deocoder but we will see that it's easy to implement the masked attention when we have implemented the basic MultiHeadAttention block.\n",
    "To implement the basic MultiheadAttention block, we will first implement the Self Attention block and see that MultiHeadAttention can be implemented as a direct extension of the Self Attention block.\n",
    "\n",
    "## Self Attention Block\n",
    "\n",
    "Taking inspiration from information retreival paradigm, Transformers have this notion of *query*, *key*, and *value* where given a *query* we try extract information from *key*-*value* pairs. Moving along those lines, we perform this mathematically by taking the weighted sum of *values* for each *query*, where weight is computed by dot product of *query* and the *key*. More precisely, for each query we compute the dot product with all the keys and then use the scalar output of those dot products as weights to find the weighted sum of *values*. Note that before finding the weighted sum, we also apply softmax function to the weights vector. Lets start with implementing of Attention Block that takes input as *query*, *key*, and *value* vectors and returns a Tensor, that is weighted sum of the *values*.\n",
    "\n",
    "For this section, you need to implement three functions, `scaled_dot_product_two_loop_single`, `scaled_dot_product_two_loop_batch`, and `scaled_dot_product_no_loop_batch` inside the transformers.py file. This might look very similar to the `dot_product_attention` in the RNN notebook but there is a subtle difference in the inputs. You should see the errors of the order less than 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "747894ed",
   "metadata": {
    "id": "747894ed"
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    scaled_dot_product_two_loop_single,\n",
    "    scaled_dot_product_two_loop_batch,\n",
    "    scaled_dot_product_no_loop_batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7341f6ff",
   "metadata": {
    "id": "7341f6ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sacled_dot_product_two_loop_single error:  5.204997336388729e-06\n"
     ]
    }
   ],
   "source": [
    "N = 2  # Number of sentences\n",
    "K = 5  # Number of words in a sentence\n",
    "M = 4  # feature dimension of each word embedding\n",
    "\n",
    "query = torch.linspace(-0.4, 0.6, steps=K * M).reshape(K, M)  # **to_double_cuda\n",
    "key = torch.linspace(-0.8, 0.5, steps=K * M).reshape(K, M)  # **to_double_cuda\n",
    "value = torch.linspace(-0.3, 0.8, steps=K * M).reshape(K, M)  # *to_double_cuda\n",
    "\n",
    "y = scaled_dot_product_two_loop_single(query, key, value)\n",
    "y_expected = torch.tensor(\n",
    "    [\n",
    "        [0.08283, 0.14073, 0.19862, 0.25652],\n",
    "        [0.13518, 0.19308, 0.25097, 0.30887],\n",
    "        [0.18848, 0.24637, 0.30427, 0.36216],\n",
    "        [0.24091, 0.29881, 0.35670, 0.41460],\n",
    "        [0.29081, 0.34871, 0.40660, 0.46450],\n",
    "    ]\n",
    ").to(torch.float32)\n",
    "print(\"sacled_dot_product_two_loop_single error: \", rel_error(y_expected, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79b11340",
   "metadata": {
    "id": "79b11340"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled_dot_product_two_loop_batch error:  4.020571992067902e-06\n"
     ]
    }
   ],
   "source": [
    "N = 2  # Number of sentences\n",
    "K = 5  # Number of words in a sentence\n",
    "M = 4  # feature dimension of each word embedding\n",
    "\n",
    "query = torch.linspace(-0.4, 0.6, steps=N * K * M).reshape(N, K, M)  # **to_double_cuda\n",
    "key = torch.linspace(-0.8, 0.5, steps=N * K * M).reshape(N, K, M)  # **to_double_cuda\n",
    "value = torch.linspace(-0.3, 0.8, steps=N * K * M).reshape(N, K, M)  # *to_double_cuda\n",
    "\n",
    "y = scaled_dot_product_two_loop_batch(query, key, value)\n",
    "y_expected = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [-0.09603, -0.06782, -0.03962, -0.01141],\n",
    "            [-0.08991, -0.06170, -0.03350, -0.00529],\n",
    "            [-0.08376, -0.05556, -0.02735, 0.00085],\n",
    "            [-0.07760, -0.04939, -0.02119, 0.00702],\n",
    "            [-0.07143, -0.04322, -0.01502, 0.01319],\n",
    "        ],\n",
    "        [\n",
    "            [0.49884, 0.52705, 0.55525, 0.58346],\n",
    "            [0.50499, 0.53319, 0.56140, 0.58960],\n",
    "            [0.51111, 0.53931, 0.56752, 0.59572],\n",
    "            [0.51718, 0.54539, 0.57359, 0.60180],\n",
    "            [0.52321, 0.55141, 0.57962, 0.60782],\n",
    "        ],\n",
    "    ]\n",
    ").to(torch.float32)\n",
    "print(\"scaled_dot_product_two_loop_batch error: \", rel_error(y_expected, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b13b2d96",
   "metadata": {
    "id": "b13b2d96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled_dot_product_no_loop_batch error:  4.020571992067902e-06\n"
     ]
    }
   ],
   "source": [
    "N = 2  # Number of sentences\n",
    "K = 5  # Number of words in a sentence\n",
    "M = 4  # feature dimension of each word embedding\n",
    "\n",
    "query = torch.linspace(-0.4, 0.6, steps=N * K * M).reshape(N, K, M)  # **to_double_cuda\n",
    "key = torch.linspace(-0.8, 0.5, steps=N * K * M).reshape(N, K, M)  # **to_double_cuda\n",
    "value = torch.linspace(-0.3, 0.8, steps=N * K * M).reshape(N, K, M)  # *to_double_cuda\n",
    "\n",
    "\n",
    "y, _ = scaled_dot_product_no_loop_batch(query, key, value)\n",
    "\n",
    "y_expected = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [-0.09603, -0.06782, -0.03962, -0.01141],\n",
    "            [-0.08991, -0.06170, -0.03350, -0.00529],\n",
    "            [-0.08376, -0.05556, -0.02735, 0.00085],\n",
    "            [-0.07760, -0.04939, -0.02119, 0.00702],\n",
    "            [-0.07143, -0.04322, -0.01502, 0.01319],\n",
    "        ],\n",
    "        [\n",
    "            [0.49884, 0.52705, 0.55525, 0.58346],\n",
    "            [0.50499, 0.53319, 0.56140, 0.58960],\n",
    "            [0.51111, 0.53931, 0.56752, 0.59572],\n",
    "            [0.51718, 0.54539, 0.57359, 0.60180],\n",
    "            [0.52321, 0.55141, 0.57962, 0.60782],\n",
    "        ],\n",
    "    ]\n",
    ").to(torch.float32)\n",
    "\n",
    "print(\"scaled_dot_product_no_loop_batch error: \", rel_error(y_expected, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e21e2d",
   "metadata": {
    "id": "56e21e2d"
   },
   "source": [
    "## Observing time complexity:\n",
    "\n",
    "As Transformers are infamous for their time complexity that depends on the size of the input sequence.\n",
    "We can verify this now that we have implemented `self_attention_no_loop`.\n",
    "Run the cells below: the first has a sequence length of 256 and the second one has a sequence length of 512. You should roughly be 4 times slower with sequence length 512, hence showing that compleixity of the transformers increase quadratically with resprect to increase in the in sequence length.\n",
    "The `%timeit` lines may take several seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7d58596",
   "metadata": {
    "id": "e7d58596"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 ms ± 3.98 ms per loop (mean ± std. dev. of 2 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "N = 64\n",
    "K = 256  # defines the input sequence length\n",
    "M = emb_size = 2048\n",
    "dim_q = dim_k = 2048\n",
    "query = torch.linspace(-0.4, 0.6, steps=N * K * M).reshape(N, K, M)  # **to_double_cuda\n",
    "key = torch.linspace(-0.8, 0.5, steps=N * K * M).reshape(N, K, M)  # **to_double_cuda\n",
    "value = torch.linspace(-0.3, 0.8, steps=N * K * M).reshape(N, K, M)  # *to_double_cuda\n",
    "\n",
    "%timeit -n 5 -r 2  y = scaled_dot_product_no_loop_batch(query, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a85adf69",
   "metadata": {
    "id": "a85adf69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433 ms ± 9.14 ms per loop (mean ± std. dev. of 2 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "N = 64\n",
    "K = 512  # defines the input requence length\n",
    "M = emb_size = 2048\n",
    "dim_q = dim_k = 2048\n",
    "query = torch.linspace(-0.4, 0.6, steps=N * K * M).reshape(N, K, M)  # **to_double_cuda\n",
    "key = torch.linspace(-0.8, 0.5, steps=N * K * M).reshape(N, K, M)  # **to_double_cuda\n",
    "value = torch.linspace(-0.3, 0.8, steps=N * K * M).reshape(N, K, M)  # *to_double_cuda\n",
    "\n",
    "%timeit -n 5 -r 2  y = scaled_dot_product_no_loop_batch(query, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6e92b0",
   "metadata": {
    "id": "9d6e92b0"
   },
   "source": [
    "Now that we have implemented `scaled_dot_product_no_loop_batch`, lets implement `SingleHeadAttention`, that will serve as a building block for the `MultiHeadAttention` block. For this exercise, we have made a `SingleHeadAttention` class that inherits from `nn.module` class of Pytorch. You need to implement the `__init__` and the `forward` functions inside `Transformers.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5af8c01-dc3a-4cca-a479-e07580914e4c",
   "metadata": {
    "id": "d5af8c01-dc3a-4cca-a479-e07580914e4c"
   },
   "source": [
    "Run the following cells to test your implementation of `SelfAttention` layer. We have also written code to check the backward pass using pytorch autograd API in the following cell. You should expect the error to be less than 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "113b3bf5",
   "metadata": {
    "id": "113b3bf5"
   },
   "outputs": [],
   "source": [
    "from transformers import SelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc3669f2",
   "metadata": {
    "id": "dc3669f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttention error:  5.282987963847609e-07\n",
      "SelfAttention error:  2.474069076879365e-06\n"
     ]
    }
   ],
   "source": [
    "reset_seed(0)\n",
    "N = 2\n",
    "K = 4\n",
    "M = emb_size = 4\n",
    "dim_q = dim_k = 4\n",
    "atten_single = SelfAttention(emb_size, dim_q, dim_k)\n",
    "\n",
    "for k, v in atten_single.named_parameters():\n",
    "    # print(k, v.shape) # uncomment this to see the weight shape\n",
    "    v.data.copy_(torch.linspace(-1.4, 1.3, steps=v.numel()).reshape(*v.shape))\n",
    "\n",
    "query = torch.linspace(-0.4, 0.6, steps=N * K * M, requires_grad=True).reshape(\n",
    "    N, K, M\n",
    ")  # **to_double_cuda\n",
    "key = torch.linspace(-0.8, 0.5, steps=N * K * M, requires_grad=True).reshape(\n",
    "    N, K, M\n",
    ")  # **to_double_cuda\n",
    "value = torch.linspace(-0.3, 0.8, steps=N * K * M, requires_grad=True).reshape(\n",
    "    N, K, M\n",
    ")  # *to_double_cuda\n",
    "\n",
    "query.retain_grad()\n",
    "key.retain_grad()\n",
    "value.retain_grad()\n",
    "\n",
    "y_expected = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [-1.10382, -0.37219, 0.35944, 1.09108],\n",
    "            [-1.45792, -0.50067, 0.45658, 1.41384],\n",
    "            [-1.74349, -0.60428, 0.53493, 1.67414],\n",
    "            [-1.92584, -0.67044, 0.58495, 1.84035],\n",
    "        ],\n",
    "        [\n",
    "            [-4.59671, -1.63952, 1.31767, 4.27486],\n",
    "            [-4.65586, -1.66098, 1.33390, 4.32877],\n",
    "            [-4.69005, -1.67339, 1.34328, 4.35994],\n",
    "            [-4.71039, -1.68077, 1.34886, 4.37848],\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "\n",
    "dy_expected = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [-0.09084, -0.08961, -0.08838, -0.08715],\n",
    "            [0.69305, 0.68366, 0.67426, 0.66487],\n",
    "            [-0.88989, -0.87783, -0.86576, -0.85370],\n",
    "            [0.25859, 0.25509, 0.25158, 0.24808],\n",
    "        ],\n",
    "        [\n",
    "            [-0.05360, -0.05287, -0.05214, -0.05142],\n",
    "            [0.11627, 0.11470, 0.11312, 0.11154],\n",
    "            [-0.01048, -0.01034, -0.01019, -0.01005],\n",
    "            [-0.03908, -0.03855, -0.03802, -0.03749],\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "\n",
    "y = atten_single(query, key, value)\n",
    "dy = torch.randn(*y.shape)  # , **to_double_cuda\n",
    "\n",
    "y.backward(dy)\n",
    "query_grad = query.grad\n",
    "\n",
    "print(\"SelfAttention error: \", rel_error(y_expected, y))\n",
    "print(\"SelfAttention error: \", rel_error(dy_expected, query_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b80b79",
   "metadata": {
    "id": "f4b80b79"
   },
   "source": [
    "We have implemented the `SingleHeadAttention` block which brings use very close to implementing `MultiHeadAttention`. We will now see that this can be achieved by manipulating the shapes of input tensors based on number of heads in the Multi-Attention block. We design a network that uses multiple SingleHeadAttention blocks on the same input to compute the output tensors and finally concatenate them to generate a single output. This is not the implementation used in practice as it forces you to initialize multiple layers but we use it here for simplicity. Implement MultiHeadAttention block in the `transformers.py` file by using the SingleHeadAttention block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d530dc6-cf5d-41b9-b62e-71a1095a0f01",
   "metadata": {
    "id": "3d530dc6-cf5d-41b9-b62e-71a1095a0f01"
   },
   "source": [
    "Run the following cells to test your `MultiHeadAttention` layer. Again, as `SelfAttention`, we have used pytorch autograd API to test the backward pass. You should expect error values below 1e-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6da2558e",
   "metadata": {
    "id": "6da2558e"
   },
   "outputs": [],
   "source": [
    "from transformers import MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87bf04ff",
   "metadata": {
    "id": "87bf04ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadAttention error:  4.6737552647256526e-07\n",
      "MultiHeadAttention error:  1.1854997386228327e-06\n"
     ]
    }
   ],
   "source": [
    "reset_seed(0)\n",
    "N = 2\n",
    "num_heads = 2\n",
    "K = 4\n",
    "M = inp_emb_size = 4\n",
    "out_emb_size = 8\n",
    "atten_multihead = MultiHeadAttention(num_heads, inp_emb_size, out_emb_size)\n",
    "\n",
    "for k, v in atten_multihead.named_parameters():\n",
    "    # print(k, v.shape) # uncomment this to see the weight shape\n",
    "    v.data.copy_(torch.linspace(-1.4, 1.3, steps=v.numel()).reshape(*v.shape))\n",
    "\n",
    "query = torch.linspace(-0.4, 0.6, steps=N * K * M, requires_grad=True).reshape(\n",
    "    N, K, M\n",
    ")  # **to_double_cuda\n",
    "key = torch.linspace(-0.8, 0.5, steps=N * K * M, requires_grad=True).reshape(\n",
    "    N, K, M\n",
    ")  # **to_double_cuda\n",
    "value = torch.linspace(-0.3, 0.8, steps=N * K * M, requires_grad=True).reshape(\n",
    "    N, K, M\n",
    ")  # *to_double_cuda\n",
    "\n",
    "query.retain_grad()\n",
    "key.retain_grad()\n",
    "value.retain_grad()\n",
    "\n",
    "y_expected = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [-0.23104, 0.50132, 1.23367, 1.96603],\n",
    "            [0.68324, 1.17869, 1.67413, 2.16958],\n",
    "            [1.40236, 1.71147, 2.02058, 2.32969],\n",
    "            [1.77330, 1.98629, 2.19928, 2.41227],\n",
    "        ],\n",
    "        [\n",
    "            [6.74946, 5.67302, 4.59659, 3.52015],\n",
    "            [6.82813, 5.73131, 4.63449, 3.53767],\n",
    "            [6.86686, 5.76001, 4.65315, 3.54630],\n",
    "            [6.88665, 5.77466, 4.66268, 3.55070],\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "dy_expected = torch.tensor(\n",
    "    [[[ 0.56268,  0.55889,  0.55510,  0.55131],\n",
    "         [ 0.43286,  0.42994,  0.42702,  0.42411],\n",
    "         [ 2.29865,  2.28316,  2.26767,  2.25218],\n",
    "         [ 0.49172,  0.48841,  0.48509,  0.48178]],\n",
    "\n",
    "        [[ 0.25083,  0.24914,  0.24745,  0.24576],\n",
    "         [ 0.14949,  0.14849,  0.14748,  0.14647],\n",
    "         [-0.03105, -0.03084, -0.03063, -0.03043],\n",
    "         [-0.02082, -0.02068, -0.02054, -0.02040]]]\n",
    ")\n",
    "\n",
    "y = atten_multihead(query, key, value)\n",
    "dy = torch.randn(*y.shape)  # , **to_double_cuda\n",
    "\n",
    "y.backward(dy)\n",
    "query_grad = query.grad\n",
    "print(\"MultiHeadAttention error: \", rel_error(y_expected, y))\n",
    "print(\"MultiHeadAttention error: \", rel_error(dy_expected, query_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be6374e-a968-4b56-a1e4-31d745330b62",
   "metadata": {
    "id": "9be6374e-a968-4b56-a1e4-31d745330b62"
   },
   "source": [
    "### LayerNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a71878d-f75b-4412-ad89-5cde3100f669",
   "metadata": {
    "id": "1a71878d-f75b-4412-ad89-5cde3100f669"
   },
   "source": [
    "In the follwing image we have highlighted the portion where LayerNorm has been used in the Transformer model. Note that in the architecture diagram it's written Add & Norm but we will implement The Norm layer for now and implement the Add part in a different manner.\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1baDniYuRzsEGnDegAFiARMhoxJjKSF2r\" alt=\"Layer_norm\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524ff12e",
   "metadata": {
    "id": "524ff12e"
   },
   "source": [
    "We implemented BatchNorm while working with CNNs. One of the problems of BatchNorm is its dependency on the the complete batch which might not give good results when the batch size is small. Ba et al proposed `LayerNormalization` that takes into account these problems and has become a standard in sequence-to-sequence tasks. In this section, we will implement `LayerNormalization`. Another nice quality of `LayerNormalization` is that as it depends on individual time steps or each element of the sequence, it can be parallelized and the test time runs in a similar manner hence making it better implementation wise. Again, you have to only implement the forward pass and the backward pass will be taken care by Pytorch autograd. Implement the `LayerNormalization` class in `transformers.py`, you should expect the error below 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdbc1bf9",
   "metadata": {
    "id": "cdbc1bf9"
   },
   "outputs": [],
   "source": [
    "from transformers import LayerNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdf95ed8",
   "metadata": {
    "id": "cdf95ed8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNormalization error:  1.3772273765080196e-06\n",
      "LayerNormalization grad error:  2.0542348921649034e-07\n"
     ]
    }
   ],
   "source": [
    "reset_seed(0)\n",
    "N = 2\n",
    "K = 4\n",
    "norm = LayerNormalization(K)\n",
    "inp = torch.linspace(-0.4, 0.6, steps=N * K, requires_grad=True).reshape(N, K)\n",
    "\n",
    "inp.retain_grad()\n",
    "y = norm(inp)\n",
    "\n",
    "y_expected = torch.tensor(\n",
    "    [[-1.34164, -0.44721, 0.44721, 1.34164], [-1.34164, -0.44721, 0.44721, 1.34164]]\n",
    ")\n",
    "\n",
    "dy_expected = torch.tensor(\n",
    "    [[  5.70524,  -2.77289, -11.56993,   8.63758],\n",
    "        [  2.26242,  -4.44330,   2.09933,   0.08154]]\n",
    ")\n",
    "\n",
    "dy = torch.randn(*y.shape)\n",
    "y.backward(dy)\n",
    "inp_grad = inp.grad\n",
    "\n",
    "print(\"LayerNormalization error: \", rel_error(y_expected, y))\n",
    "print(\"LayerNormalization grad error: \", rel_error(dy_expected, inp_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e86954f-f7df-4d7f-ab6c-2fe1dcb1f5b0",
   "metadata": {
    "id": "5e86954f-f7df-4d7f-ab6c-2fe1dcb1f5b0"
   },
   "source": [
    "### FeedForward Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3404d48c-95ce-47d6-ae27-93590552e221",
   "metadata": {
    "id": "3404d48c-95ce-47d6-ae27-93590552e221"
   },
   "source": [
    "In the image below we have highlighted the parts where FeedForward Block is used.\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1WCNACnI-Q6OfU3ngjIMCbNzb1sbFnCgP\" alt=\"Layer_norm\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49a145",
   "metadata": {
    "id": "5d49a145"
   },
   "source": [
    "Next, we will implement the `Feedforward` block. These are used in both the Encoder and Decoder network of the Transformer and they consist of stacked MLP and ReLU layers. In the overall architecture, the output of `MultiHeadAttention` is fed into the `FeedForward` block. Implement the `FeedForwardBlock` inside `transformers.py` and execute the following cells to check your implementation. You should expect the errors below 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc2118cc",
   "metadata": {
    "id": "cc2118cc"
   },
   "outputs": [],
   "source": [
    "from transformers import FeedForwardBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "764da0f7",
   "metadata": {
    "id": "764da0f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForwardBlock error:  2.1976864847460601e-07\n",
      "FeedForwardBlock error:  2.302209602061989e-06\n"
     ]
    }
   ],
   "source": [
    "reset_seed(0)\n",
    "N = 2\n",
    "K = 4\n",
    "M = emb_size = 4\n",
    "\n",
    "ff_block = FeedForwardBlock(emb_size, 2 * emb_size)\n",
    "\n",
    "for k, v in ff_block.named_parameters():\n",
    "    v.data.copy_(torch.linspace(-1.4, 1.3, steps=v.numel()).reshape(*v.shape))\n",
    "\n",
    "inp = torch.linspace(-0.4, 0.6, steps=N * K, requires_grad=True).reshape(\n",
    "    N, K\n",
    ") \n",
    "inp.retain_grad()\n",
    "y = ff_block(inp)\n",
    "\n",
    "y_expected = torch.tensor(\n",
    "    [[-2.46161, -0.71662, 1.02838, 2.77337], [-7.56084, -1.69557, 4.16970, 10.03497]]\n",
    ")\n",
    "\n",
    "dy_expected = torch.tensor(\n",
    "    [[0.55105, 0.68884, 0.82662, 0.96441], [0.30734, 0.31821, 0.32908, 0.33996]]\n",
    ")\n",
    "\n",
    "dy = torch.randn(*y.shape)\n",
    "y.backward(dy)\n",
    "inp_grad = inp.grad\n",
    "\n",
    "print(\"FeedForwardBlock error: \", rel_error(y_expected, y))\n",
    "print(\"FeedForwardBlock error: \", rel_error(dy_expected, inp_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09489f50",
   "metadata": {
    "id": "09489f50"
   },
   "source": [
    "Now, if you look back to the original paper, Attention is all you Need, then, we are almost done with the building blocks of a transformer. What's left is:\n",
    "\n",
    "- Encapsulating the building blocks into Encoder Block\n",
    "- Encapsulating the building blocks into Decoder Block\n",
    "- Handling the input data preprocessing and positional encoding.\n",
    "\n",
    "We will first look at implementing the Encoder Block and Decoder block. The positional encoding is a non learnable embedding and we can treat it as a preprocessing step in our DataLoader."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaf4c99-49ba-49d0-970a-a8ee0ce3656e",
   "metadata": {
    "id": "ccaf4c99-49ba-49d0-970a-a8ee0ce3656e"
   },
   "source": [
    "In the figure below we have highlighted the encoder block in a Transformer. Notice that it is build using all the components we already implemented before. We just have to be careful about \n",
    "the residual connections in various blocks.\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1LsTN1BapktFzSo0smWV881kKeeJRfAa_\" alt=\"Layer_norm\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1364689-db62-44f4-85ca-ed005a357f54",
   "metadata": {
    "id": "c1364689-db62-44f4-85ca-ed005a357f54"
   },
   "source": [
    "As shown in the figure above, the encoder block takes it inputs three tensors. We will assume that we have those three tensors, query, key, and value. Run the cell below to check your implementation of the EncoderBlock. You should expect the errors below 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9fa5fea",
   "metadata": {
    "id": "b9fa5fea"
   },
   "outputs": [],
   "source": [
    "from transformers import EncoderBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76e8ccb0",
   "metadata": {
    "id": "76e8ccb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderBlock error 1:  5.951869338703975e-07\n",
      "EncoderBlock error 2:  6.382652540503625e-07\n"
     ]
    }
   ],
   "source": [
    "reset_seed(0)\n",
    "N = 2\n",
    "num_heads = 2\n",
    "emb_dim = K = 4\n",
    "feedforward_dim = 8\n",
    "M = inp_emb_size = 4\n",
    "out_emb_size = 8\n",
    "dropout = 0.2\n",
    "\n",
    "enc_seq_inp = torch.linspace(-0.4, 0.6, steps=N * K * M, requires_grad=True).reshape(\n",
    "    N, K, M\n",
    ")  # **to_double_cuda\n",
    "\n",
    "enc_block = EncoderBlock(num_heads, emb_dim, feedforward_dim, dropout)\n",
    "\n",
    "for k, v in enc_block.named_parameters():\n",
    "    # print(k, v.shape) # uncomment this to see the weight shape\n",
    "    v.data.copy_(torch.linspace(-1.4, 1.3, steps=v.numel()).reshape(*v.shape))\n",
    "\n",
    "encoder_out1_expected = torch.tensor(\n",
    "    [[[ 0.00000, -0.31357,  0.69126,  0.00000],\n",
    "         [ 0.42630, -0.25859,  0.72412,  3.87013],\n",
    "         [ 0.00000, -0.31357,  0.69126,  3.89884],\n",
    "         [ 0.47986, -0.30568,  0.69082,  3.90563]],\n",
    "\n",
    "        [[ 0.00000, -0.31641,  0.69000,  3.89921],\n",
    "         [ 0.47986, -0.30568,  0.69082,  3.90563],\n",
    "         [ 0.47986, -0.30568,  0.69082,  3.90563],\n",
    "         [ 0.51781, -0.30853,  0.71598,  3.85171]]]\n",
    ")\n",
    "encoder_out1 = enc_block(enc_seq_inp)\n",
    "print(\"EncoderBlock error 1: \", rel_error(encoder_out1, encoder_out1_expected))\n",
    "\n",
    "\n",
    "N = 2\n",
    "num_heads = 1\n",
    "emb_dim = K = 4\n",
    "feedforward_dim = 8\n",
    "M = inp_emb_size = 4\n",
    "out_emb_size = 8\n",
    "dropout = 0.2\n",
    "\n",
    "enc_seq_inp = torch.linspace(-0.4, 0.6, steps=N * K * M, requires_grad=True).reshape(\n",
    "    N, K, M\n",
    ")  # **to_double_cuda\n",
    "\n",
    "enc_block = EncoderBlock(num_heads, emb_dim, feedforward_dim, dropout)\n",
    "\n",
    "for k, v in enc_block.named_parameters():\n",
    "    # print(k, v.shape) # uncomment this to see the weight shape\n",
    "    v.data.copy_(torch.linspace(-1.4, 1.3, steps=v.numel()).reshape(*v.shape))\n",
    "\n",
    "encoder_out2_expected = torch.tensor(\n",
    "    [[[ 0.42630, -0.00000,  0.72412,  3.87013],\n",
    "         [ 0.49614, -0.31357,  0.00000,  3.89884],\n",
    "         [ 0.47986, -0.30568,  0.69082,  0.00000],\n",
    "         [ 0.51654, -0.32455,  0.69035,  3.89216]],\n",
    "\n",
    "        [[ 0.47986, -0.30568,  0.69082,  0.00000],\n",
    "         [ 0.49614, -0.31357,  0.69126,  3.89884],\n",
    "         [ 0.00000, -0.30354,  0.76272,  3.75311],\n",
    "         [ 0.49614, -0.31357,  0.69126,  3.89884]]]\n",
    ")\n",
    "encoder_out2 = enc_block(enc_seq_inp)\n",
    "print(\"EncoderBlock error 2: \", rel_error(encoder_out2, encoder_out2_expected))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc029c5b-ca52-454a-b8a4-b07cea708b5e",
   "metadata": {
    "id": "cc029c5b-ca52-454a-b8a4-b07cea708b5e"
   },
   "source": [
    "Great! You're almost done with the implementation of the Transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2616ef-5934-4a50-8f51-642ef635e2cb",
   "metadata": {
    "id": "4b2616ef-5934-4a50-8f51-642ef635e2cb"
   },
   "source": [
    "### Decoder Block\n",
    "\n",
    "The image below shows the highlighted Decoder block. Notice how it takes the input from the encoder and the target sequence.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1DwU3BJsA0mUWTWlXNtNolB4oc5K4Z9PE\" alt=\"Layer_norm\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e2d5d7-6918-4ea8-a369-0d53304ff1a7",
   "metadata": {
    "id": "19e2d5d7-6918-4ea8-a369-0d53304ff1a7"
   },
   "source": [
    "Now, we will look at the implementation of the decoder. In the  class we learned about encoder only model that can be used for tasks like sequence classification but for more complicated tasks like sequence to sequence we need a decoder network that can transform the output of the encoder to a target sequence. This kind of architecture is important in tasks like language translation where we have a sequence as input and a sequence as output. This decoder takes the input from the encoder and the previous generated value to generate the next value. During training, we use a Mask on the input so that the decoder network can't look ahead in the future and during inference we sequentially process the data.\n",
    "\n",
    "Before moving to implementing the Decoder Block, we should pay attention to the figure above. It says a \"Masked MultiHead Attention\" which actually prevents the decoder from looking ahead into the future. Lets understand with an example here. We have an expression as `BOS POSITIVE 01 add POSITIVE 00 EOS`, i.e. `1+0` that gives output as `BOS POSITIVE 01 EOS`, i.e. `+1`. Lets focus on the output sequence here. This is a sequence of length 5 (after applying our preprocessing code) and will will get transformed into *key*, *query*, and *value* matrix of dimension $5\\times128$, $5\\times128$ and $5\\times128$ respectively, where 128 is the embedding dimension of the Transformer. Now, while training, we input these vectors in the `self_attention_no_loop_batch` without mask. It will compute the dot product between *query* and *key* to generate a $5\\times5$ matrix where the first row (shape $1\\times5$) of that matrix tells us how much the word `EOS` is related with `EOS`, `POSITIVE`, `0`, `1`, and `EOS`. This means that it will use the weights of all these tokens to learn the final sequence that is to be predicted. This is okay when we are training the model but what happens when we perform inference? We start with a brand new expression, input this expression in the encoder but this time we only have the first starting token `EOS` for decoder and we don't know about the rest of the tokens in the sequence. Hence, a solution to this problem is to mask the weights inside the function `self_attention_no_loop_batch` for only the decoder part. This masking should prevent the decoder from accessing the future or next elements.\n",
    "\n",
    "We will now look at how to generate this mask for a given sequence. Then, you should also update the `self_attention_no_loop_batch` to use the mask variable appropriately. Implement the `get_subsequent_mask`, `self_attention_no_loop_batch` with mask inside `transformers.py` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39392320-f75a-49cc-9d90-364d5eab897b",
   "metadata": {
    "id": "39392320-f75a-49cc-9d90-364d5eab897b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_subsequent_mask error:  0.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_subsequent_mask\n",
    "\n",
    "reset_seed(0)\n",
    "seq_len_enc = K = 4\n",
    "M = inp_emb_size = 3\n",
    "\n",
    "inp_sequence = torch.linspace(-0.4, 0.6, steps=K * M, requires_grad=True).reshape(\n",
    "    K, M\n",
    ")  # **to_double_cuda\n",
    "\n",
    "mask_expected = torch.tensor(\n",
    "    [\n",
    "        [[False, True, True], [False, False, True], [False, False, False]],\n",
    "        [[False, True, True], [False, False, True], [False, False, False]],\n",
    "        [[False, True, True], [False, False, True], [False, False, False]],\n",
    "        [[False, True, True], [False, False, True], [False, False, False]],\n",
    "    ]\n",
    ")\n",
    "mask_predicted = get_subsequent_mask(inp_sequence)\n",
    "# print(\"mask_expected: \", mask_expected)\n",
    "# print(\"mask_predicted: \", mask_predicted)\n",
    "\n",
    "print(\n",
    "    \"get_subsequent_mask error: \", rel_error(mask_predicted.int(), mask_expected.int())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "408e7333-9d14-498d-8661-b9f8703b24fe",
   "metadata": {
    "id": "408e7333-9d14-498d-8661-b9f8703b24fe"
   },
   "outputs": [],
   "source": [
    "from transformers import scaled_dot_product_no_loop_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b6d6114-f8fa-484b-8084-ea078db1545a",
   "metadata": {
    "id": "0b6d6114-f8fa-484b-8084-ea078db1545a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled_dot_product_no_loop_batch error:  2.8390648478191238e-06\n"
     ]
    }
   ],
   "source": [
    "reset_seed(0)\n",
    "N = 4\n",
    "K = 3\n",
    "M = 3\n",
    "\n",
    "query = torch.linspace(-0.4, 0.6, steps=K * M * N, requires_grad=True).reshape(N, K, M)\n",
    "key = torch.linspace(-0.1, 0.2, steps=K * M * N, requires_grad=True).reshape(N, K, M)\n",
    "value = torch.linspace(0.4, 0.8, steps=K * M * N, requires_grad=True).reshape(N, K, M)\n",
    "\n",
    "y_expected = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [0.40000, 0.41143, 0.42286],\n",
    "            [0.41703, 0.42846, 0.43989],\n",
    "            [0.43408, 0.44551, 0.45694],\n",
    "        ],\n",
    "        [\n",
    "            [0.50286, 0.51429, 0.52571],\n",
    "            [0.51999, 0.53142, 0.54285],\n",
    "            [0.53720, 0.54863, 0.56006],\n",
    "        ],\n",
    "        [\n",
    "            [0.60571, 0.61714, 0.62857],\n",
    "            [0.62294, 0.63437, 0.64580],\n",
    "            [0.64032, 0.65175, 0.66318],\n",
    "        ],\n",
    "        [\n",
    "            [0.70857, 0.72000, 0.73143],\n",
    "            [0.72590, 0.73733, 0.74876],\n",
    "            [0.74344, 0.75487, 0.76630],\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "y_predicted, _ = scaled_dot_product_no_loop_batch(query, key, value, mask_expected)\n",
    "\n",
    "print(\"scaled_dot_product_no_loop_batch error: \", rel_error(y_expected, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9671dba4-bf00-434d-865d-97c967e154a6",
   "metadata": {
    "id": "9671dba4-bf00-434d-865d-97c967e154a6"
   },
   "source": [
    "Lets finally implement the decoder block now that we have all the required tools to implement it. Fill in the init function and the forward pass of the `DecoderBlock` inside `transformers.py`. Run the following cells to check your implementation of the `DecoderBlock`. You should expect the errors below 1e-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa350b69",
   "metadata": {
    "id": "aa350b69"
   },
   "outputs": [],
   "source": [
    "from transformers import DecoderBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62927d47",
   "metadata": {
    "id": "62927d47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecoderBlock error:  0.501587873365352\n",
      "DecoderBlock error:  0.4970657771765451\n"
     ]
    }
   ],
   "source": [
    "reset_seed(0)\n",
    "N = 2\n",
    "num_heads = 2\n",
    "seq_len_enc = K1 = 4\n",
    "seq_len_dec = K2 = 2\n",
    "feedforward_dim = 8\n",
    "M = emb_dim = 4\n",
    "out_emb_size = 8\n",
    "dropout = 0.2\n",
    "\n",
    "dec_inp = torch.linspace(-0.4, 0.6, steps=N * K1 * M, requires_grad=True).reshape(\n",
    "    N, K1, M\n",
    ")\n",
    "enc_out = torch.linspace(-0.4, 0.6, steps=N * K2 * M, requires_grad=True).reshape(\n",
    "    N, K2, M\n",
    ")\n",
    "dec_block = DecoderBlock(num_heads, emb_dim, feedforward_dim, dropout)\n",
    "\n",
    "for k, v in dec_block.named_parameters():\n",
    "    # print(k, v.shape) # uncomment this to see the weight shape\n",
    "    v.data.copy_(torch.linspace(-1.4, 1.3, steps=v.numel()).reshape(*v.shape))\n",
    "\n",
    "\n",
    "dec_out_expected = torch.tensor(\n",
    "    [[[ 0.50623, -0.32496,  0.00000,  0.00000],\n",
    "         [ 0.00000, -0.31690,  0.76956,  3.72647],\n",
    "         [ 0.49014, -0.32809,  0.66595,  3.93773],\n",
    "         [ 0.00000, -0.00000,  0.68203,  3.90856]],\n",
    "\n",
    "        [[ 0.51042, -0.32787,  0.68093,  3.90848],\n",
    "         [ 0.00000, -0.31637,  0.72275,  3.83122],\n",
    "         [ 0.64868, -0.00000,  0.77715,  0.00000],\n",
    "         [ 0.00000, -0.33105,  0.66565,  3.93602]]]\n",
    ")\n",
    "dec_out1 = dec_block(dec_inp, enc_out)\n",
    "print(\"DecoderBlock error: \", rel_error(dec_out1, dec_out_expected))\n",
    "\n",
    "N = 2\n",
    "num_heads = 2\n",
    "seq_len_enc = K1 = 4\n",
    "seq_len_dec = K2 = 4\n",
    "feedforward_dim = 4\n",
    "M = emb_dim = 4\n",
    "out_emb_size = 8\n",
    "dropout = 0.2\n",
    "\n",
    "dec_inp = torch.linspace(-0.4, 0.6, steps=N * K1 * M, requires_grad=True).reshape(\n",
    "    N, K1, M\n",
    ")  \n",
    "enc_out = torch.linspace(-0.4, 0.6, steps=N * K2 * M, requires_grad=True).reshape(\n",
    "    N, K2, M\n",
    ")  \n",
    "dec_block = DecoderBlock(num_heads, emb_dim, feedforward_dim, dropout)\n",
    "\n",
    "for k, v in dec_block.named_parameters():\n",
    "    # print(k, v.shape) # uncomment this to see the weight shape\n",
    "    v.data.copy_(torch.linspace(-1.4, 1.3, steps=v.numel()).reshape(*v.shape))\n",
    "\n",
    "\n",
    "dec_out_expected = torch.tensor(\n",
    "    [[[ 0.46707, -0.31916,  0.66218,  3.95182],\n",
    "         [ 0.00000, -0.31116,  0.66325,  0.00000],\n",
    "         [ 0.44538, -0.32419,  0.64068,  3.98847],\n",
    "         [ 0.49012, -0.31276,  0.68795,  3.90610]],\n",
    "\n",
    "        [[ 0.45800, -0.33023,  0.64106,  3.98324],\n",
    "         [ 0.45829, -0.31487,  0.66203,  3.95529],\n",
    "         [ 0.59787, -0.00000,  0.72361,  0.00000],\n",
    "         [ 0.70958, -0.37051,  0.78886,  3.63179]]]\n",
    ")\n",
    "dec_out2 = dec_block(dec_inp, enc_out)\n",
    "print(\"DecoderBlock error: \", rel_error(dec_out2, dec_out_expected))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba0c23d-ba32-442d-87fe-26c1f9848aad",
   "metadata": {
    "id": "cba0c23d-ba32-442d-87fe-26c1f9848aad"
   },
   "source": [
    "Based on the implementation of `EncoderBlock` and `DecoderBlock`, we have implemented the `Encoder` and `Decoder` networks for you in transformers.py. You should be able to understand the input and outputs of these Encoder and Decoder blocks. Implement the Transformer block inside transformer.py using these networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a41e64-4de5-4289-be47-3e7282e88f35",
   "metadata": {
    "id": "20a41e64-4de5-4289-be47-3e7282e88f35"
   },
   "source": [
    "## Part III: Data loader\n",
    "\n",
    "In this part, we will have a look at creating the final data loader for the task, that can be used to train the Transformer model. This will comprise of two things:\n",
    "\n",
    "- Implement Positional Encoding\n",
    "- Create a dataloader using the `prepocess_input_sequence` fucntion that we created in Part I."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1249f651-4de3-4dd7-aa85-7d8bfb3e1e4f",
   "metadata": {
    "id": "1249f651-4de3-4dd7-aa85-7d8bfb3e1e4f"
   },
   "source": [
    "Lets start with implementing the Positional Encoding for the input. The positional encodings make the Transformers positionally aware about sequences. These are usually added to the input and hence should be same shape as input. As these are not learnable, they remain constant throughtout the training process. For this reason, we can look at it as a pre-processing step that's done on the input. Our strategy here would be to implement positional encoding function and use it later while creating DataLoader for the toy dataset.\n",
    "\n",
    "Lets look at the simplest kind of positional encoding, i.e. for a sequence of length K, assign the nth element in the sequence a value of n/K, where n starts from 0. Implement the position_encoding_simple inside `transformers.py`. You should expect error less than 1e-9 here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0798f4f1-fdb5-4e3b-9dba-5e1e1d64277b",
   "metadata": {
    "id": "0798f4f1-fdb5-4e3b-9dba-5e1e1d64277b"
   },
   "source": [
    "### Simple positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90e1dad2-aecb-487e-8ece-c23b7e48c291",
   "metadata": {
    "id": "90e1dad2-aecb-487e-8ece-c23b7e48c291"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position_encoding_simple error:  0.0\n",
      "position_encoding_simple error:  0.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import position_encoding_simple\n",
    "\n",
    "reset_seed(0)\n",
    "K = 4\n",
    "M = emb_size = 4\n",
    "\n",
    "y = position_encoding_simple(K, M)\n",
    "y_expected = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [0.00000, 0.00000, 0.00000, 0.00000],\n",
    "            [0.25000, 0.25000, 0.25000, 0.25000],\n",
    "            [0.50000, 0.50000, 0.50000, 0.50000],\n",
    "            [0.75000, 0.75000, 0.75000, 0.75000],\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"position_encoding_simple error: \", rel_error(y, y_expected))\n",
    "\n",
    "K = 5\n",
    "M = emb_size = 3\n",
    "\n",
    "\n",
    "y = position_encoding_simple(K, M)\n",
    "y_expected = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [0.00000, 0.00000, 0.00000],\n",
    "            [0.20000, 0.20000, 0.20000],\n",
    "            [0.40000, 0.40000, 0.40000],\n",
    "            [0.60000, 0.60000, 0.60000],\n",
    "            [0.80000, 0.80000, 0.80000],\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "print(\"position_encoding_simple error: \", rel_error(y, y_expected))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ddbbc-1631-4781-849d-3fd27a3b0619",
   "metadata": {
    "id": "8c8ddbbc-1631-4781-849d-3fd27a3b0619"
   },
   "source": [
    "### Sinusoid positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3db3a0",
   "metadata": {
    "id": "1e3db3a0"
   },
   "source": [
    "Now that we have looked at a simple positional encoding, we can see one major drawback, which is that if the sequence length gets larger, the difference between two consecutive positional encodings becomes smaller and smaller and it in turn defeats a purpose of positional awareness, as there is very small diference in two consecutive positions. Another issue is that for each position we replicated it along embedding dimension, hence introducing redundancy which might not help the network in learning anything new. There could be different tricks that can be used to make a positional encoding that could solve these problems.\n",
    "\n",
    "Lets look at more mature version of a positonal encoding that uses a combination of sines and cosines function, also called sinusoid. This is also the positional encoding used in the original Transformer paper. For each element in the sequence (length K) with position $p$ and embedding (dimension M) positon $i$, we can define the positional encoding as:\n",
    "\n",
    "$$PE_{(p, 2i)} = \\sin\\left(\\frac{p}{10000^a}\\right)$$\n",
    "$$PE_{(p, 2i+1)} = \\cos\\left(\\frac{p}{10000^a}\\right)$$\n",
    "\n",
    "$$\\text{Where }a = \\left\\lfloor{\\frac{2i}{M}}\\right\\rfloor \\text{and M is the Embedding dimension of the Transformer}$$\n",
    "\n",
    "Here, $p$ remains constant for a position in the sequence and we assign alternating sines and cosines along the embedding dimension.\n",
    "\n",
    "Implement the fucntion `position_encoding` inside `transformers.py`. You should expect errors below 1e-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "135f8367-b90d-4956-8783-1e0c8e3e2c94",
   "metadata": {
    "id": "135f8367-b90d-4956-8783-1e0c8e3e2c94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position_encoding error:  1.5795230865478516e-06\n",
      "position_encoding error:  1.817941665649414e-06\n"
     ]
    }
   ],
   "source": [
    "from transformers import position_encoding_sinusoid\n",
    "\n",
    "reset_seed(0)\n",
    "K = 4\n",
    "M = emb_size = 4\n",
    "\n",
    "y1 = position_encoding_sinusoid(K, M)\n",
    "y_expected = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [0.00000, 1.00000, 0.00000, 1.00000],\n",
    "            [0.84147, 0.54030, 0.84147, 0.54030],\n",
    "            [0.90930, -0.41615, 0.90930, -0.41615],\n",
    "            [0.14112, -0.98999, 0.14112, -0.98999],\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "# print('y1: ', y1)\n",
    "# print('y_expected: ', y_expected)\n",
    "print(\"position_encoding error: \", rel_error(y1, y_expected))\n",
    "\n",
    "K = 5\n",
    "M = emb_size = 3\n",
    "\n",
    "\n",
    "y2 = position_encoding_sinusoid(K, M)\n",
    "y_expected = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [0.00000, 1.00000, 0.00000],\n",
    "            [0.84147, 0.54030, 0.84147],\n",
    "            [0.90930, -0.41615, 0.90930],\n",
    "            [0.14112, -0.98999, 0.14112],\n",
    "            [-0.75680, -0.65364, -0.75680],\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "print(\"position_encoding error: \", rel_error(y2, y_expected))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579e6f2e-d2f0-4a10-a54f-c65ecdccc587",
   "metadata": {
    "id": "579e6f2e-d2f0-4a10-a54f-c65ecdccc587"
   },
   "source": [
    "### Constructing the DataLoader for the toy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2d8dd1",
   "metadata": {
    "id": "5f2d8dd1"
   },
   "source": [
    "Now we will use the implemented positonal encodings to construct a DataLoader in Pytorch. The function of a data loader is to return a batch for training/validation. We first make a Dataset class that gives us a single element in the batch and then use a DataLoader to wrap the dataset. We inherit the Dataset from `torch.utils.data.Dataset` class. This class consists of two important functions that you'd change depending on your usecase (for e.g. the upcoming project!). The first function is `__init__`, this consists of the components that are *static*, in other words, these are the variables that won't change when we want the next element from the complete data. The second fucntion is `__getitem__` which contains the core functionality of the final dataloader.\n",
    "\n",
    "To get the final dataloader, we wrap the `train_data` and `test_data` in `torch.utils.data.DataLoader` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a673ee9f-c5ae-438c-b696-9662dcb87c52",
   "metadata": {
    "id": "a673ee9f-c5ae-438c-b696-9662dcb87c52"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AddSubDataset\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "X, y = data[\"inp_expression\"], data[\"out_expression\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "train_data = AddSubDataset(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    convert_str_to_tokens,\n",
    "    SPECIAL_TOKENS,\n",
    "    32,\n",
    "    position_encoding_simple,\n",
    ")\n",
    "valid_data = AddSubDataset(\n",
    "    X_test, y_test, convert_str_to_tokens, SPECIAL_TOKENS, 32, position_encoding_simple\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ac4856-63fd-4693-87b2-ae616f438202",
   "metadata": {
    "id": "17ac4856-63fd-4693-87b2-ae616f438202"
   },
   "source": [
    "## Part IV: Using transformer on the toy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fde5c5d",
   "metadata": {
    "id": "0fde5c5d"
   },
   "source": [
    "In this part, we will put all the parts together to train a transformer model. We have implemented most of the functions here for you and your task would be to use these functions to train a Transformer model. The overall tasks are divided into three parts:\n",
    "\n",
    "- Implement the Transformer model using previusly implemented functions\n",
    "- Overfitting the model\n",
    "- Training using complete data\n",
    "- Visualizing the attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a14ddc39",
   "metadata": {
    "id": "a14ddc39"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0918d6d-0207-406e-9f2e-9a9cc901754f",
   "metadata": {
    "id": "c0918d6d-0207-406e-9f2e-9a9cc901754f"
   },
   "source": [
    "### Implement the Transformer model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f77aa51-af4e-404b-a981-75edd5892623",
   "metadata": {
    "id": "3f77aa51-af4e-404b-a981-75edd5892623"
   },
   "source": [
    "We will add all the peices together to implement the Transformer model completely, as shown in the figure below. Note that till now we have implemented the Encoder and Decoder, and we handled the positional encodings for the input. Whats left is the input and output embedding layer. We will share this embedding layer for the encoder and decoder here. Lastly, we need to map the final output of the decoder to the vocabulary length(the last linear block after decoder in the figure below)\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1snyWKrr2r1J-O8VQTVxkwQYptk0oFhIM\" alt=\"Layer_norm\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7447b1ff",
   "metadata": {
    "id": "7447b1ff"
   },
   "source": [
    "Implement the Transformer model in `transformer.py` and run the cells below to check the final shapes of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d0f19cf",
   "metadata": {
    "id": "8d0f19cf"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from transformers import Transformer\n",
    "\n",
    "inp_seq_len = 9\n",
    "out_seq_len = 5\n",
    "num_heads = 4\n",
    "emb_dim = 32\n",
    "dim_feedforward = 64\n",
    "dropout = 0.2\n",
    "num_enc_layers = 4\n",
    "num_dec_layers = 4\n",
    "vocab_len = len(vocab)\n",
    "\n",
    "model = Transformer(\n",
    "    num_heads,\n",
    "    emb_dim,\n",
    "    dim_feedforward,\n",
    "    dropout,\n",
    "    num_enc_layers,\n",
    "    num_dec_layers,\n",
    "    vocab_len,\n",
    ")\n",
    "for it in train_loader:\n",
    "  it\n",
    "  break\n",
    "inp, inp_pos, out, out_pos = it\n",
    "\n",
    "\n",
    "device = DEVICE\n",
    "model = model.to(device)\n",
    "inp_pos = inp_pos.to(device)\n",
    "out_pos = out_pos.to(device)\n",
    "out = out.to(device)\n",
    "inp = inp.to(device)\n",
    "# print(\"inp shape: \", inp.shape)\n",
    "# print(\"inp_pos shape: \", inp_pos.shape)\n",
    "# print(\"out shape: \", out.shape)\n",
    "# print(\"out_pos shape: \", out_pos.shape)\n",
    "\n",
    "\n",
    "model_out = model(inp.long(), inp_pos, out.long(), out_pos)\n",
    "assert model_out.size(0) == BATCH_SIZE * (out_seq_len - 1)\n",
    "assert model_out.size(1) == vocab_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc890c7f",
   "metadata": {
    "id": "bc890c7f"
   },
   "source": [
    "### Overfitting the model using small data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a4b8da",
   "metadata": {
    "id": "62a4b8da"
   },
   "source": [
    "Now that we have implemented the Transformer model, lets overfit on a small dataset. This will ensure that the implementation is correct. We keep the training and validation data same here. Before doing that, a couple of things to keep in mind:\n",
    "\n",
    "- We implemented two versions of positional encodings: simple and sinusoid. For overfitting, we will use the simple positional encoding but feel free to experiment with both when training for the complete model\n",
    "- In transformers.py, we have implemented two loss functions for you. The first is the familiar cross entropy loss and second is the `LabelSmoothingLoss`. For overfitting, we will use the cross entropy loss but feel free to experiment with both while doing experiment with the complete data.\n",
    "- Usually, the training regime of Transformers start with a warmup, in other words, we train the model with a lower learning rate for some iterations and then increasing the learning rate to make the network learn faster. Intuitively, this helps you to attain a stable manifold in the loss function and then we increase the learning rate to learn faster in this stable manifold. In a way we are warming up the network to be in a stable manifold and we start training with a higher learning rate after this warm-up. For overfitting we have NOT used this warm-up as for such small data, it is okay to start with a higer learning rate but you should keep this in mind while training with the complete data. We have used two functions from a5_helper.py, `train` and `val`. Here, `train` has three parameters that you should pay attention to:\n",
    "  - `warmup_interval`: Specifies the number of iterations that the network should train with a low learning rate. In other words, its the number of iterations after which the network will have the higher learning rate\n",
    "  - `warmup_lr`: This is the learning rate that will be used during warmup.\n",
    "  - `lr`: This is the learning rate that will get used after the warm-up. If warmup_interval is None, we will start training with this learning rate.\n",
    "\n",
    "In the following cells for overfitting, we have used the number of epochs as 200 but you could increase this. You should get an accuracy ~1 in 200 epochs. It might be a little lower as well, don't worry about it. It should take about a minute to run the overfitting.\n",
    "\n",
    "NOTE: When we say epoch, it means the number of times we have taken a complete pass over the data. One epoch typically consists of many iterations that depend on the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f9a206d1",
   "metadata": {
    "id": "f9a206d1"
   },
   "outputs": [],
   "source": [
    "from transformers import LabelSmoothingLoss, CrossEntropyLoss\n",
    "import torch.optim as optim\n",
    "from transformers import Transformer\n",
    "from a5_helper import train as train_transformer\n",
    "from a5_helper import val as val_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "033af962",
   "metadata": {
    "id": "033af962"
   },
   "outputs": [],
   "source": [
    "inp_seq_len = 9\n",
    "out_seq_len = 5\n",
    "num_heads = 4\n",
    "emb_dim = 32\n",
    "dim_feedforward = 32\n",
    "dropout = 0.2\n",
    "num_enc_layers = 1\n",
    "num_dec_layers = 1\n",
    "vocab_len = len(vocab)\n",
    "BATCH_SIZE = 4\n",
    "num_epochs=200 #number of epochs\n",
    "lr=1e-3 #learning rate after warmup\n",
    "loss_func = CrossEntropyLoss\n",
    "warmup_interval = None #number of iterations for warmup\n",
    "\n",
    "model = Transformer(\n",
    "    num_heads,\n",
    "    emb_dim,\n",
    "    dim_feedforward,\n",
    "    dropout,\n",
    "    num_enc_layers,\n",
    "    num_dec_layers,\n",
    "    vocab_len,\n",
    ")\n",
    "train_data = AddSubDataset(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    convert_str_to_tokens,\n",
    "    SPECIAL_TOKENS,\n",
    "    emb_dim,\n",
    "    position_encoding_simple,\n",
    ")\n",
    "valid_data = AddSubDataset(\n",
    "    X_test,\n",
    "    y_test,\n",
    "    convert_str_to_tokens,\n",
    "    SPECIAL_TOKENS,\n",
    "    emb_dim,\n",
    "    position_encoding_simple,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True\n",
    ")\n",
    "\n",
    "small_dataset = torch.utils.data.Subset(\n",
    "    train_data, torch.linspace(0, len(train_data) - 1, steps=4).long()\n",
    ")\n",
    "small_train_loader = torch.utils.data.DataLoader(\n",
    "    small_dataset, batch_size=4, pin_memory=True, num_workers=1, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c4fa6c5",
   "metadata": {
    "id": "8c4fa6c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n",
      "[epoch: 1] [loss:  3.6266 ] val_loss: [val_loss  3.2411 ]\n",
      "[epoch: 2] [loss:  3.3738 ] val_loss: [val_loss  3.0595 ]\n",
      "[epoch: 3] [loss:  3.0950 ] val_loss: [val_loss  2.8972 ]\n",
      "[epoch: 4] [loss:  3.0141 ] val_loss: [val_loss  2.7381 ]\n",
      "[epoch: 5] [loss:  3.4979 ] val_loss: [val_loss  2.5925 ]\n",
      "[epoch: 6] [loss:  3.0216 ] val_loss: [val_loss  2.4561 ]\n",
      "[epoch: 7] [loss:  2.7352 ] val_loss: [val_loss  2.3312 ]\n",
      "[epoch: 8] [loss:  3.4108 ] val_loss: [val_loss  2.2178 ]\n",
      "[epoch: 9] [loss:  2.4946 ] val_loss: [val_loss  2.1178 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 10] [loss:  2.3746 ] val_loss: [val_loss  2.0256 ]\n",
      "[epoch: 11] [loss:  2.4249 ] val_loss: [val_loss  1.9439 ]\n",
      "[epoch: 12] [loss:  2.4657 ] val_loss: [val_loss  1.8727 ]\n",
      "[epoch: 13] [loss:  1.9837 ] val_loss: [val_loss  1.8090 ]\n",
      "[epoch: 14] [loss:  2.2593 ] val_loss: [val_loss  1.7522 ]\n",
      "[epoch: 15] [loss:  2.2748 ] val_loss: [val_loss  1.7006 ]\n",
      "[epoch: 16] [loss:  2.1372 ] val_loss: [val_loss  1.6519 ]\n",
      "[epoch: 17] [loss:  2.0571 ] val_loss: [val_loss  1.6077 ]\n",
      "[epoch: 18] [loss:  1.8973 ] val_loss: [val_loss  1.5690 ]\n",
      "[epoch: 19] [loss:  1.9436 ] val_loss: [val_loss  1.5353 ]\n",
      "[epoch: 20] [loss:  2.2504 ] val_loss: [val_loss  1.5004 ]\n",
      "[epoch: 21] [loss:  2.0955 ] val_loss: [val_loss  1.4657 ]\n",
      "[epoch: 22] [loss:  1.7349 ] val_loss: [val_loss  1.4314 ]\n",
      "[epoch: 23] [loss:  2.0516 ] val_loss: [val_loss  1.3970 ]\n",
      "[epoch: 24] [loss:  1.8001 ] val_loss: [val_loss  1.3626 ]\n",
      "[epoch: 25] [loss:  1.9558 ] val_loss: [val_loss  1.3267 ]\n",
      "[epoch: 26] [loss:  1.7105 ] val_loss: [val_loss  1.2851 ]\n",
      "[epoch: 27] [loss:  1.5096 ] val_loss: [val_loss  1.2401 ]\n",
      "[epoch: 28] [loss:  1.9449 ] val_loss: [val_loss  1.1966 ]\n",
      "[epoch: 29] [loss:  1.5525 ] val_loss: [val_loss  1.1535 ]\n",
      "[epoch: 30] [loss:  1.5414 ] val_loss: [val_loss  1.1121 ]\n",
      "[epoch: 31] [loss:  1.5153 ] val_loss: [val_loss  1.0743 ]\n",
      "[epoch: 32] [loss:  1.4891 ] val_loss: [val_loss  1.0395 ]\n",
      "[epoch: 33] [loss:  1.4673 ] val_loss: [val_loss  1.0075 ]\n",
      "[epoch: 34] [loss:  1.4122 ] val_loss: [val_loss  0.9761 ]\n",
      "[epoch: 35] [loss:  1.5449 ] val_loss: [val_loss  0.9465 ]\n",
      "[epoch: 36] [loss:  1.3968 ] val_loss: [val_loss  0.9157 ]\n",
      "[epoch: 37] [loss:  1.6858 ] val_loss: [val_loss  0.8852 ]\n",
      "[epoch: 38] [loss:  1.3363 ] val_loss: [val_loss  0.8549 ]\n",
      "[epoch: 39] [loss:  1.4336 ] val_loss: [val_loss  0.8250 ]\n",
      "[epoch: 40] [loss:  1.5151 ] val_loss: [val_loss  0.7954 ]\n",
      "[epoch: 41] [loss:  1.2991 ] val_loss: [val_loss  0.7661 ]\n",
      "[epoch: 42] [loss:  1.2620 ] val_loss: [val_loss  0.7366 ]\n",
      "[epoch: 43] [loss:  1.1558 ] val_loss: [val_loss  0.7076 ]\n",
      "[epoch: 44] [loss:  1.0533 ] val_loss: [val_loss  0.6786 ]\n",
      "[epoch: 45] [loss:  1.1928 ] val_loss: [val_loss  0.6502 ]\n",
      "[epoch: 46] [loss:  1.2722 ] val_loss: [val_loss  0.6231 ]\n",
      "[epoch: 47] [loss:  1.3555 ] val_loss: [val_loss  0.5974 ]\n",
      "[epoch: 48] [loss:  1.0248 ] val_loss: [val_loss  0.5761 ]\n",
      "[epoch: 49] [loss:  1.3235 ] val_loss: [val_loss  0.5545 ]\n",
      "[epoch: 50] [loss:  1.0998 ] val_loss: [val_loss  0.5341 ]\n",
      "[epoch: 51] [loss:  0.9621 ] val_loss: [val_loss  0.5154 ]\n",
      "[epoch: 52] [loss:  0.9843 ] val_loss: [val_loss  0.4960 ]\n",
      "[epoch: 53] [loss:  1.0454 ] val_loss: [val_loss  0.4783 ]\n",
      "[epoch: 54] [loss:  0.9600 ] val_loss: [val_loss  0.4625 ]\n",
      "[epoch: 55] [loss:  0.9064 ] val_loss: [val_loss  0.4475 ]\n",
      "[epoch: 56] [loss:  1.0147 ] val_loss: [val_loss  0.4343 ]\n",
      "[epoch: 57] [loss:  1.0512 ] val_loss: [val_loss  0.4211 ]\n",
      "[epoch: 58] [loss:  0.9367 ] val_loss: [val_loss  0.4087 ]\n",
      "[epoch: 59] [loss:  0.7173 ] val_loss: [val_loss  0.3972 ]\n",
      "[epoch: 60] [loss:  0.7687 ] val_loss: [val_loss  0.3870 ]\n",
      "[epoch: 61] [loss:  0.9525 ] val_loss: [val_loss  0.3808 ]\n",
      "[epoch: 62] [loss:  0.7940 ] val_loss: [val_loss  0.3742 ]\n",
      "[epoch: 63] [loss:  0.9022 ] val_loss: [val_loss  0.3688 ]\n",
      "[epoch: 64] [loss:  0.7995 ] val_loss: [val_loss  0.3626 ]\n",
      "[epoch: 65] [loss:  0.7564 ] val_loss: [val_loss  0.3567 ]\n",
      "[epoch: 66] [loss:  0.6334 ] val_loss: [val_loss  0.3516 ]\n",
      "[epoch: 67] [loss:  0.7573 ] val_loss: [val_loss  0.3414 ]\n",
      "[epoch: 68] [loss:  0.8408 ] val_loss: [val_loss  0.3263 ]\n",
      "[epoch: 69] [loss:  0.6603 ] val_loss: [val_loss  0.3155 ]\n",
      "[epoch: 70] [loss:  0.8744 ] val_loss: [val_loss  0.3044 ]\n",
      "[epoch: 71] [loss:  0.8538 ] val_loss: [val_loss  0.2926 ]\n",
      "[epoch: 72] [loss:  0.6044 ] val_loss: [val_loss  0.2823 ]\n",
      "[epoch: 73] [loss:  0.5261 ] val_loss: [val_loss  0.2749 ]\n",
      "[epoch: 74] [loss:  0.8680 ] val_loss: [val_loss  0.2655 ]\n",
      "[epoch: 75] [loss:  0.8533 ] val_loss: [val_loss  0.2563 ]\n",
      "[epoch: 76] [loss:  0.8025 ] val_loss: [val_loss  0.2480 ]\n",
      "[epoch: 77] [loss:  0.8574 ] val_loss: [val_loss  0.2424 ]\n",
      "[epoch: 78] [loss:  0.9389 ] val_loss: [val_loss  0.2359 ]\n",
      "[epoch: 79] [loss:  0.5358 ] val_loss: [val_loss  0.2295 ]\n",
      "[epoch: 80] [loss:  0.4943 ] val_loss: [val_loss  0.2226 ]\n",
      "[epoch: 81] [loss:  0.8169 ] val_loss: [val_loss  0.2154 ]\n",
      "[epoch: 82] [loss:  0.5311 ] val_loss: [val_loss  0.2109 ]\n",
      "[epoch: 83] [loss:  0.6532 ] val_loss: [val_loss  0.2064 ]\n",
      "[epoch: 84] [loss:  0.4038 ] val_loss: [val_loss  0.2031 ]\n",
      "[epoch: 85] [loss:  0.5144 ] val_loss: [val_loss  0.1930 ]\n",
      "[epoch: 86] [loss:  0.4876 ] val_loss: [val_loss  0.1843 ]\n",
      "[epoch: 87] [loss:  0.5595 ] val_loss: [val_loss  0.1763 ]\n",
      "[epoch: 88] [loss:  0.4042 ] val_loss: [val_loss  0.1692 ]\n",
      "[epoch: 89] [loss:  0.6116 ] val_loss: [val_loss  0.1654 ]\n",
      "[epoch: 90] [loss:  0.5734 ] val_loss: [val_loss  0.1655 ]\n",
      "[epoch: 91] [loss:  0.7189 ] val_loss: [val_loss  0.1719 ]\n",
      "[epoch: 92] [loss:  0.6175 ] val_loss: [val_loss  0.1804 ]\n",
      "[epoch: 93] [loss:  0.4481 ] val_loss: [val_loss  0.1867 ]\n",
      "[epoch: 94] [loss:  0.3580 ] val_loss: [val_loss  0.1939 ]\n",
      "[epoch: 95] [loss:  0.5905 ] val_loss: [val_loss  0.1980 ]\n",
      "[epoch: 96] [loss:  0.5254 ] val_loss: [val_loss  0.1869 ]\n",
      "[epoch: 97] [loss:  0.4983 ] val_loss: [val_loss  0.1687 ]\n",
      "[epoch: 98] [loss:  0.5399 ] val_loss: [val_loss  0.1516 ]\n",
      "[epoch: 99] [loss:  0.4705 ] val_loss: [val_loss  0.1403 ]\n",
      "[epoch: 100] [loss:  0.5065 ] val_loss: [val_loss  0.1302 ]\n",
      "[epoch: 101] [loss:  0.5119 ] val_loss: [val_loss  0.1249 ]\n",
      "[epoch: 102] [loss:  0.4415 ] val_loss: [val_loss  0.1231 ]\n",
      "[epoch: 103] [loss:  0.4447 ] val_loss: [val_loss  0.1263 ]\n",
      "[epoch: 104] [loss:  0.2814 ] val_loss: [val_loss  0.1321 ]\n",
      "[epoch: 105] [loss:  0.2905 ] val_loss: [val_loss  0.1389 ]\n",
      "[epoch: 106] [loss:  0.5373 ] val_loss: [val_loss  0.1449 ]\n",
      "[epoch: 107] [loss:  0.4259 ] val_loss: [val_loss  0.1450 ]\n",
      "[epoch: 108] [loss:  0.3673 ] val_loss: [val_loss  0.1448 ]\n",
      "[epoch: 109] [loss:  0.4350 ] val_loss: [val_loss  0.1422 ]\n",
      "[epoch: 110] [loss:  0.3767 ] val_loss: [val_loss  0.1385 ]\n",
      "[epoch: 111] [loss:  0.3566 ] val_loss: [val_loss  0.1314 ]\n",
      "[epoch: 112] [loss:  0.6328 ] val_loss: [val_loss  0.1203 ]\n",
      "[epoch: 113] [loss:  0.3532 ] val_loss: [val_loss  0.1132 ]\n",
      "[epoch: 114] [loss:  0.5286 ] val_loss: [val_loss  0.1061 ]\n",
      "[epoch: 115] [loss:  0.3665 ] val_loss: [val_loss  0.0994 ]\n",
      "[epoch: 116] [loss:  0.2264 ] val_loss: [val_loss  0.0944 ]\n",
      "[epoch: 117] [loss:  0.3498 ] val_loss: [val_loss  0.0911 ]\n",
      "[epoch: 118] [loss:  0.4682 ] val_loss: [val_loss  0.0877 ]\n",
      "[epoch: 119] [loss:  0.2785 ] val_loss: [val_loss  0.0842 ]\n",
      "[epoch: 120] [loss:  0.1931 ] val_loss: [val_loss  0.0821 ]\n",
      "[epoch: 121] [loss:  0.2654 ] val_loss: [val_loss  0.0799 ]\n",
      "[epoch: 122] [loss:  0.1813 ] val_loss: [val_loss  0.0778 ]\n",
      "[epoch: 123] [loss:  0.1684 ] val_loss: [val_loss  0.0760 ]\n",
      "[epoch: 124] [loss:  0.2089 ] val_loss: [val_loss  0.0742 ]\n",
      "[epoch: 125] [loss:  0.3402 ] val_loss: [val_loss  0.0725 ]\n",
      "[epoch: 126] [loss:  0.3443 ] val_loss: [val_loss  0.0709 ]\n",
      "[epoch: 127] [loss:  0.4251 ] val_loss: [val_loss  0.0695 ]\n",
      "[epoch: 128] [loss:  0.2258 ] val_loss: [val_loss  0.0680 ]\n",
      "[epoch: 129] [loss:  0.2853 ] val_loss: [val_loss  0.0666 ]\n",
      "[epoch: 130] [loss:  0.3394 ] val_loss: [val_loss  0.0648 ]\n",
      "[epoch: 131] [loss:  0.2148 ] val_loss: [val_loss  0.0631 ]\n",
      "[epoch: 132] [loss:  0.3852 ] val_loss: [val_loss  0.0618 ]\n",
      "[epoch: 133] [loss:  0.2123 ] val_loss: [val_loss  0.0606 ]\n",
      "[epoch: 134] [loss:  0.1536 ] val_loss: [val_loss  0.0597 ]\n",
      "[epoch: 135] [loss:  0.2110 ] val_loss: [val_loss  0.0587 ]\n",
      "[epoch: 136] [loss:  0.3338 ] val_loss: [val_loss  0.0568 ]\n",
      "[epoch: 137] [loss:  0.2406 ] val_loss: [val_loss  0.0551 ]\n",
      "[epoch: 138] [loss:  0.2635 ] val_loss: [val_loss  0.0535 ]\n",
      "[epoch: 139] [loss:  0.3753 ] val_loss: [val_loss  0.0521 ]\n",
      "[epoch: 140] [loss:  0.1897 ] val_loss: [val_loss  0.0510 ]\n",
      "[epoch: 141] [loss:  0.2091 ] val_loss: [val_loss  0.0501 ]\n",
      "[epoch: 142] [loss:  0.2092 ] val_loss: [val_loss  0.0494 ]\n",
      "[epoch: 143] [loss:  0.1508 ] val_loss: [val_loss  0.0490 ]\n",
      "[epoch: 144] [loss:  0.2510 ] val_loss: [val_loss  0.0479 ]\n",
      "[epoch: 145] [loss:  0.4043 ] val_loss: [val_loss  0.0459 ]\n",
      "[epoch: 146] [loss:  0.2060 ] val_loss: [val_loss  0.0444 ]\n",
      "[epoch: 147] [loss:  0.2442 ] val_loss: [val_loss  0.0432 ]\n",
      "[epoch: 148] [loss:  0.2137 ] val_loss: [val_loss  0.0419 ]\n",
      "[epoch: 149] [loss:  0.3259 ] val_loss: [val_loss  0.0408 ]\n",
      "[epoch: 150] [loss:  0.1717 ] val_loss: [val_loss  0.0397 ]\n",
      "[epoch: 151] [loss:  0.1232 ] val_loss: [val_loss  0.0388 ]\n",
      "[epoch: 152] [loss:  0.1594 ] val_loss: [val_loss  0.0379 ]\n",
      "[epoch: 153] [loss:  0.2565 ] val_loss: [val_loss  0.0373 ]\n",
      "[epoch: 154] [loss:  0.2194 ] val_loss: [val_loss  0.0366 ]\n",
      "[epoch: 155] [loss:  0.2483 ] val_loss: [val_loss  0.0360 ]\n",
      "[epoch: 156] [loss:  0.1947 ] val_loss: [val_loss  0.0354 ]\n",
      "[epoch: 157] [loss:  0.3148 ] val_loss: [val_loss  0.0343 ]\n",
      "[epoch: 158] [loss:  0.1860 ] val_loss: [val_loss  0.0334 ]\n",
      "[epoch: 159] [loss:  0.2082 ] val_loss: [val_loss  0.0328 ]\n",
      "[epoch: 160] [loss:  0.1075 ] val_loss: [val_loss  0.0324 ]\n",
      "[epoch: 161] [loss:  0.1056 ] val_loss: [val_loss  0.0318 ]\n",
      "[epoch: 162] [loss:  0.3750 ] val_loss: [val_loss  0.0313 ]\n",
      "[epoch: 163] [loss:  0.1191 ] val_loss: [val_loss  0.0309 ]\n",
      "[epoch: 164] [loss:  0.1363 ] val_loss: [val_loss  0.0304 ]\n",
      "[epoch: 165] [loss:  0.2496 ] val_loss: [val_loss  0.0298 ]\n",
      "[epoch: 166] [loss:  0.1687 ] val_loss: [val_loss  0.0293 ]\n",
      "[epoch: 167] [loss:  0.2273 ] val_loss: [val_loss  0.0288 ]\n",
      "[epoch: 168] [loss:  0.1642 ] val_loss: [val_loss  0.0283 ]\n",
      "[epoch: 169] [loss:  0.3469 ] val_loss: [val_loss  0.0278 ]\n",
      "[epoch: 170] [loss:  0.3285 ] val_loss: [val_loss  0.0274 ]\n",
      "[epoch: 171] [loss:  0.1585 ] val_loss: [val_loss  0.0271 ]\n",
      "[epoch: 172] [loss:  0.1495 ] val_loss: [val_loss  0.0268 ]\n",
      "[epoch: 173] [loss:  0.1617 ] val_loss: [val_loss  0.0264 ]\n",
      "[epoch: 174] [loss:  0.1902 ] val_loss: [val_loss  0.0260 ]\n",
      "[epoch: 175] [loss:  0.1688 ] val_loss: [val_loss  0.0257 ]\n",
      "[epoch: 176] [loss:  0.1423 ] val_loss: [val_loss  0.0253 ]\n",
      "[epoch: 177] [loss:  0.1136 ] val_loss: [val_loss  0.0250 ]\n",
      "[epoch: 178] [loss:  0.2215 ] val_loss: [val_loss  0.0246 ]\n",
      "[epoch: 179] [loss:  0.1495 ] val_loss: [val_loss  0.0242 ]\n",
      "[epoch: 180] [loss:  0.1823 ] val_loss: [val_loss  0.0240 ]\n",
      "[epoch: 181] [loss:  0.1322 ] val_loss: [val_loss  0.0238 ]\n",
      "[epoch: 182] [loss:  0.1558 ] val_loss: [val_loss  0.0238 ]\n",
      "[epoch: 183] [loss:  0.0691 ] val_loss: [val_loss  0.0238 ]\n",
      "[epoch: 184] [loss:  0.2109 ] val_loss: [val_loss  0.0238 ]\n",
      "[epoch: 185] [loss:  0.1110 ] val_loss: [val_loss  0.0240 ]\n",
      "[epoch: 186] [loss:  0.1822 ] val_loss: [val_loss  0.0235 ]\n",
      "[epoch: 187] [loss:  0.1632 ] val_loss: [val_loss  0.0230 ]\n",
      "[epoch: 188] [loss:  0.0916 ] val_loss: [val_loss  0.0226 ]\n",
      "[epoch: 189] [loss:  0.1041 ] val_loss: [val_loss  0.0221 ]\n",
      "[epoch: 190] [loss:  0.1144 ] val_loss: [val_loss  0.0216 ]\n",
      "[epoch: 191] [loss:  0.0953 ] val_loss: [val_loss  0.0212 ]\n",
      "[epoch: 192] [loss:  0.0821 ] val_loss: [val_loss  0.0209 ]\n",
      "[epoch: 193] [loss:  0.1882 ] val_loss: [val_loss  0.0206 ]\n",
      "[epoch: 194] [loss:  0.0973 ] val_loss: [val_loss  0.0204 ]\n",
      "[epoch: 195] [loss:  0.1128 ] val_loss: [val_loss  0.0201 ]\n",
      "[epoch: 196] [loss:  0.3145 ] val_loss: [val_loss  0.0199 ]\n",
      "[epoch: 197] [loss:  0.1766 ] val_loss: [val_loss  0.0196 ]\n",
      "[epoch: 198] [loss:  0.1098 ] val_loss: [val_loss  0.0193 ]\n",
      "[epoch: 199] [loss:  0.1387 ] val_loss: [val_loss  0.0191 ]\n",
      "[epoch: 200] [loss:  0.1341 ] val_loss: [val_loss  0.0185 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Overfitting the model\n",
    "trained_model = train_transformer(\n",
    "    model,\n",
    "    small_train_loader,\n",
    "    small_train_loader,\n",
    "    loss_func,\n",
    "    num_epochs=num_epochs,\n",
    "    lr=lr,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    warmup_interval=warmup_interval,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "21a3a5eb",
   "metadata": {
    "id": "21a3a5eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overfitted accuracy:  1.0000\n"
     ]
    }
   ],
   "source": [
    "#Overfitted accuracy\n",
    "print(\n",
    "    \"Overfitted accuracy: \",\n",
    "    \"{:.4f}\".format(\n",
    "        val_transformer(\n",
    "            trained_model,\n",
    "            small_train_loader,\n",
    "            CrossEntropyLoss,\n",
    "            batch_size=4,\n",
    "            device=DEVICE,\n",
    "        )[1]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43b5e66-7d96-49a7-8d73-649c1d8de2ef",
   "metadata": {
    "id": "b43b5e66-7d96-49a7-8d73-649c1d8de2ef"
   },
   "source": [
    "### Fitting the model using complete data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3f4cfc-d267-436e-ae7a-36e3ab40e7c1",
   "metadata": {
    "id": "3f3f4cfc-d267-436e-ae7a-36e3ab40e7c1"
   },
   "source": [
    "Run the below cells to fit the model using the complete data. Keep in mind the various things you could experiment with here, losses, positional encodings, warm up routines and learning rates. You could also play with the size of the model but that will require more time to train on Colab.\n",
    "\n",
    "You should aim for final validation accuracy of ~80 percent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "352896e6",
   "metadata": {
    "id": "352896e6"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from transformers import Transformer\n",
    "\n",
    "inp_seq_len = 9\n",
    "out_seq_len = 5\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "#You should change these!\n",
    "\n",
    "# num_heads = 4\n",
    "# emb_dim = 32\n",
    "# dim_feedforward = 32\n",
    "# dropout = 0.2\n",
    "# num_enc_layers = 4\n",
    "# num_dec_layers = 4\n",
    "# vocab_len = len(vocab)\n",
    "# loss_func = CrossEntropyLoss\n",
    "# poss_enc = position_encoding_simple\n",
    "# num_epochs = 200\n",
    "# warmup_interval = None\n",
    "# lr = 1e-3\n",
    "\n",
    "num_heads = 4\n",
    "emb_dim = 256\n",
    "dim_feedforward = 256\n",
    "dropout = 0.2\n",
    "num_enc_layers = 2\n",
    "num_dec_layers = 2\n",
    "vocab_len = len(vocab)\n",
    "loss_func = CrossEntropyLoss\n",
    "poss_enc = position_encoding_simple\n",
    "num_epochs = 200\n",
    "warmup_interval = None\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "model = Transformer(\n",
    "    num_heads,\n",
    "    emb_dim,\n",
    "    dim_feedforward,\n",
    "    dropout,\n",
    "    num_enc_layers,\n",
    "    num_dec_layers,\n",
    "    vocab_len,\n",
    ")\n",
    "\n",
    "\n",
    "train_data = AddSubDataset(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    convert_str_to_tokens,\n",
    "    SPECIAL_TOKENS,\n",
    "    emb_dim,\n",
    "    position_encoding_sinusoid,\n",
    ")\n",
    "valid_data = AddSubDataset(\n",
    "    X_test,\n",
    "    y_test,\n",
    "    convert_str_to_tokens,\n",
    "    SPECIAL_TOKENS,\n",
    "    emb_dim,\n",
    "    position_encoding_sinusoid,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "43bfb054",
   "metadata": {
    "id": "43bfb054"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n",
      "[epoch: 1] [loss:  2.8016 ] val_loss: [val_loss  1.7317 ]\n",
      "[epoch: 2] [loss:  1.6350 ] val_loss: [val_loss  1.2675 ]\n",
      "[epoch: 3] [loss:  1.3750 ] val_loss: [val_loss  1.1823 ]\n",
      "[epoch: 4] [loss:  1.2888 ] val_loss: [val_loss  1.1461 ]\n",
      "[epoch: 5] [loss:  1.2432 ] val_loss: [val_loss  1.1121 ]\n",
      "[epoch: 6] [loss:  1.1917 ] val_loss: [val_loss  1.0723 ]\n",
      "[epoch: 7] [loss:  1.1252 ] val_loss: [val_loss  1.0169 ]\n",
      "[epoch: 8] [loss:  1.0654 ] val_loss: [val_loss  0.9654 ]\n",
      "[epoch: 9] [loss:  1.0200 ] val_loss: [val_loss  0.9059 ]\n",
      "[epoch: 10] [loss:  0.9736 ] val_loss: [val_loss  0.8609 ]\n",
      "[epoch: 11] [loss:  0.9331 ] val_loss: [val_loss  0.8524 ]\n",
      "[epoch: 12] [loss:  0.8943 ] val_loss: [val_loss  0.8129 ]\n",
      "[epoch: 13] [loss:  0.8678 ] val_loss: [val_loss  0.8061 ]\n",
      "[epoch: 14] [loss:  0.8502 ] val_loss: [val_loss  0.7741 ]\n",
      "[epoch: 15] [loss:  0.8271 ] val_loss: [val_loss  0.7673 ]\n",
      "[epoch: 16] [loss:  0.8150 ] val_loss: [val_loss  0.7465 ]\n",
      "[epoch: 17] [loss:  0.7973 ] val_loss: [val_loss  0.7626 ]\n",
      "[epoch: 18] [loss:  0.7960 ] val_loss: [val_loss  0.7274 ]\n",
      "[epoch: 19] [loss:  0.7904 ] val_loss: [val_loss  0.7230 ]\n",
      "[epoch: 20] [loss:  0.7828 ] val_loss: [val_loss  0.7289 ]\n",
      "[epoch: 21] [loss:  0.7699 ] val_loss: [val_loss  0.7310 ]\n",
      "[epoch: 22] [loss:  0.7695 ] val_loss: [val_loss  0.7460 ]\n",
      "[epoch: 23] [loss:  0.7560 ] val_loss: [val_loss  0.7139 ]\n",
      "[epoch: 24] [loss:  0.7454 ] val_loss: [val_loss  0.7112 ]\n",
      "[epoch: 25] [loss:  0.7420 ] val_loss: [val_loss  0.7055 ]\n",
      "[epoch: 26] [loss:  0.7307 ] val_loss: [val_loss  0.6820 ]\n",
      "[epoch: 27] [loss:  0.7165 ] val_loss: [val_loss  0.6751 ]\n",
      "[epoch: 28] [loss:  0.7229 ] val_loss: [val_loss  0.6833 ]\n",
      "[epoch: 29] [loss:  0.7059 ] val_loss: [val_loss  0.6702 ]\n",
      "[epoch: 30] [loss:  0.7126 ] val_loss: [val_loss  0.6602 ]\n",
      "[epoch: 31] [loss:  0.7115 ] val_loss: [val_loss  0.6516 ]\n",
      "[epoch: 32] [loss:  0.7024 ] val_loss: [val_loss  0.6619 ]\n",
      "[epoch: 33] [loss:  0.6925 ] val_loss: [val_loss  0.6513 ]\n",
      "[epoch: 34] [loss:  0.6928 ] val_loss: [val_loss  0.6456 ]\n",
      "[epoch: 35] [loss:  0.7259 ] val_loss: [val_loss  0.7152 ]\n",
      "[epoch: 36] [loss:  0.7292 ] val_loss: [val_loss  0.6623 ]\n",
      "[epoch: 37] [loss:  0.7079 ] val_loss: [val_loss  0.6829 ]\n",
      "[epoch: 38] [loss:  0.7058 ] val_loss: [val_loss  0.6678 ]\n",
      "[epoch: 39] [loss:  0.7629 ] val_loss: [val_loss  0.7386 ]\n",
      "[epoch: 40] [loss:  0.7351 ] val_loss: [val_loss  0.6695 ]\n",
      "[epoch: 41] [loss:  0.6903 ] val_loss: [val_loss  0.6419 ]\n",
      "[epoch: 42] [loss:  0.6757 ] val_loss: [val_loss  0.6373 ]\n",
      "[epoch: 43] [loss:  0.6645 ] val_loss: [val_loss  0.6273 ]\n",
      "[epoch: 44] [loss:  0.6610 ] val_loss: [val_loss  0.6204 ]\n",
      "[epoch: 45] [loss:  0.6528 ] val_loss: [val_loss  0.6214 ]\n",
      "[epoch: 46] [loss:  0.6392 ] val_loss: [val_loss  0.6070 ]\n",
      "[epoch: 47] [loss:  0.6469 ] val_loss: [val_loss  0.6204 ]\n",
      "[epoch: 48] [loss:  0.6391 ] val_loss: [val_loss  0.5887 ]\n",
      "[epoch: 49] [loss:  0.6361 ] val_loss: [val_loss  0.5744 ]\n",
      "[epoch: 50] [loss:  0.6414 ] val_loss: [val_loss  0.5917 ]\n",
      "[epoch: 51] [loss:  0.6326 ] val_loss: [val_loss  0.6026 ]\n",
      "[epoch: 52] [loss:  0.6719 ] val_loss: [val_loss  0.6423 ]\n",
      "[epoch: 53] [loss:  0.6624 ] val_loss: [val_loss  0.5821 ]\n",
      "[epoch: 54] [loss:  0.6391 ] val_loss: [val_loss  0.5908 ]\n",
      "[epoch: 55] [loss:  0.6190 ] val_loss: [val_loss  0.5676 ]\n",
      "[epoch: 56] [loss:  0.6206 ] val_loss: [val_loss  0.5684 ]\n",
      "[epoch: 57] [loss:  0.6090 ] val_loss: [val_loss  0.5790 ]\n",
      "[epoch: 58] [loss:  0.6055 ] val_loss: [val_loss  0.5717 ]\n",
      "[epoch: 59] [loss:  0.5932 ] val_loss: [val_loss  0.5549 ]\n",
      "[epoch: 60] [loss:  0.5860 ] val_loss: [val_loss  0.5261 ]\n",
      "[epoch: 61] [loss:  0.5844 ] val_loss: [val_loss  0.5302 ]\n",
      "[epoch: 62] [loss:  0.5855 ] val_loss: [val_loss  0.5213 ]\n",
      "[epoch: 63] [loss:  0.5812 ] val_loss: [val_loss  0.5377 ]\n",
      "[epoch: 64] [loss:  0.5885 ] val_loss: [val_loss  0.5267 ]\n",
      "[epoch: 65] [loss:  0.5967 ] val_loss: [val_loss  0.5239 ]\n",
      "[epoch: 66] [loss:  0.5784 ] val_loss: [val_loss  0.5436 ]\n",
      "[epoch: 67] [loss:  0.5735 ] val_loss: [val_loss  0.5355 ]\n",
      "[epoch: 68] [loss:  0.5884 ] val_loss: [val_loss  0.5289 ]\n",
      "[epoch: 69] [loss:  0.5593 ] val_loss: [val_loss  0.4890 ]\n",
      "[epoch: 70] [loss:  0.5339 ] val_loss: [val_loss  0.4603 ]\n",
      "[epoch: 71] [loss:  0.5375 ] val_loss: [val_loss  0.4691 ]\n",
      "[epoch: 72] [loss:  0.5306 ] val_loss: [val_loss  0.4905 ]\n",
      "[epoch: 73] [loss:  0.5282 ] val_loss: [val_loss  0.4450 ]\n",
      "[epoch: 74] [loss:  0.5226 ] val_loss: [val_loss  0.4442 ]\n",
      "[epoch: 75] [loss:  0.5194 ] val_loss: [val_loss  0.4421 ]\n",
      "[epoch: 76] [loss:  0.5180 ] val_loss: [val_loss  0.4364 ]\n",
      "[epoch: 77] [loss:  0.4910 ] val_loss: [val_loss  0.4080 ]\n",
      "[epoch: 78] [loss:  0.4831 ] val_loss: [val_loss  0.4431 ]\n",
      "[epoch: 79] [loss:  0.4689 ] val_loss: [val_loss  0.3595 ]\n",
      "[epoch: 80] [loss:  0.4507 ] val_loss: [val_loss  0.3653 ]\n",
      "[epoch: 81] [loss:  0.4552 ] val_loss: [val_loss  0.4144 ]\n",
      "[epoch: 82] [loss:  0.4400 ] val_loss: [val_loss  0.3517 ]\n",
      "[epoch: 83] [loss:  0.4162 ] val_loss: [val_loss  0.3229 ]\n",
      "[epoch: 84] [loss:  0.4137 ] val_loss: [val_loss  0.3086 ]\n",
      "[epoch: 85] [loss:  0.3971 ] val_loss: [val_loss  0.3087 ]\n",
      "[epoch: 86] [loss:  0.4135 ] val_loss: [val_loss  0.2959 ]\n",
      "[epoch: 87] [loss:  0.3812 ] val_loss: [val_loss  0.2617 ]\n",
      "[epoch: 88] [loss:  0.3743 ] val_loss: [val_loss  0.2890 ]\n",
      "[epoch: 89] [loss:  0.3767 ] val_loss: [val_loss  0.2616 ]\n",
      "[epoch: 90] [loss:  0.3258 ] val_loss: [val_loss  0.2332 ]\n",
      "[epoch: 91] [loss:  0.3314 ] val_loss: [val_loss  0.2535 ]\n",
      "[epoch: 92] [loss:  0.3389 ] val_loss: [val_loss  0.2353 ]\n",
      "[epoch: 93] [loss:  0.3011 ] val_loss: [val_loss  0.2051 ]\n",
      "[epoch: 94] [loss:  0.2839 ] val_loss: [val_loss  0.1973 ]\n",
      "[epoch: 95] [loss:  0.2661 ] val_loss: [val_loss  0.1863 ]\n",
      "[epoch: 96] [loss:  0.2596 ] val_loss: [val_loss  0.1602 ]\n",
      "[epoch: 97] [loss:  0.2362 ] val_loss: [val_loss  0.1491 ]\n",
      "[epoch: 98] [loss:  0.2271 ] val_loss: [val_loss  0.1639 ]\n",
      "[epoch: 99] [loss:  0.2265 ] val_loss: [val_loss  0.1525 ]\n",
      "[epoch: 100] [loss:  0.2275 ] val_loss: [val_loss  0.1317 ]\n",
      "[epoch: 101] [loss:  0.2150 ] val_loss: [val_loss  0.1215 ]\n",
      "[epoch: 102] [loss:  0.1918 ] val_loss: [val_loss  0.1092 ]\n",
      "[epoch: 103] [loss:  0.1855 ] val_loss: [val_loss  0.1044 ]\n",
      "[epoch: 104] [loss:  0.2107 ] val_loss: [val_loss  0.1262 ]\n",
      "[epoch: 105] [loss:  0.2062 ] val_loss: [val_loss  0.1055 ]\n",
      "[epoch: 106] [loss:  0.1858 ] val_loss: [val_loss  0.1021 ]\n",
      "[epoch: 107] [loss:  0.1666 ] val_loss: [val_loss  0.0840 ]\n",
      "[epoch: 108] [loss:  0.1511 ] val_loss: [val_loss  0.0885 ]\n",
      "[epoch: 109] [loss:  0.1499 ] val_loss: [val_loss  0.0704 ]\n",
      "[epoch: 110] [loss:  0.1526 ] val_loss: [val_loss  0.0741 ]\n",
      "[epoch: 111] [loss:  0.1571 ] val_loss: [val_loss  0.0912 ]\n",
      "[epoch: 112] [loss:  0.1517 ] val_loss: [val_loss  0.0769 ]\n",
      "[epoch: 113] [loss:  0.1749 ] val_loss: [val_loss  0.0882 ]\n",
      "[epoch: 114] [loss:  0.1628 ] val_loss: [val_loss  0.0696 ]\n",
      "[epoch: 115] [loss:  0.1434 ] val_loss: [val_loss  0.0779 ]\n",
      "[epoch: 116] [loss:  0.1288 ] val_loss: [val_loss  0.0603 ]\n",
      "[epoch: 117] [loss:  0.1209 ] val_loss: [val_loss  0.0549 ]\n",
      "[epoch: 118] [loss:  0.1300 ] val_loss: [val_loss  0.1448 ]\n",
      "[epoch: 119] [loss:  0.1990 ] val_loss: [val_loss  0.1180 ]\n",
      "[epoch: 120] [loss:  0.1917 ] val_loss: [val_loss  0.0838 ]\n",
      "[epoch: 121] [loss:  0.1497 ] val_loss: [val_loss  0.0644 ]\n",
      "[epoch: 122] [loss:  0.1249 ] val_loss: [val_loss  0.0589 ]\n",
      "[epoch: 123] [loss:  0.1055 ] val_loss: [val_loss  0.0390 ]\n",
      "[epoch: 124] [loss:  0.0992 ] val_loss: [val_loss  0.0378 ]\n",
      "[epoch: 125] [loss:  0.0946 ] val_loss: [val_loss  0.0432 ]\n",
      "[epoch: 126] [loss:  0.0926 ] val_loss: [val_loss  0.0361 ]\n",
      "[epoch: 127] [loss:  0.0867 ] val_loss: [val_loss  0.0283 ]\n",
      "[epoch: 128] [loss:  0.0893 ] val_loss: [val_loss  0.0403 ]\n",
      "[epoch: 129] [loss:  0.0898 ] val_loss: [val_loss  0.0312 ]\n",
      "[epoch: 130] [loss:  0.0899 ] val_loss: [val_loss  0.0254 ]\n",
      "[epoch: 131] [loss:  0.0897 ] val_loss: [val_loss  0.0283 ]\n",
      "[epoch: 132] [loss:  0.1013 ] val_loss: [val_loss  0.0268 ]\n",
      "[epoch: 133] [loss:  0.0925 ] val_loss: [val_loss  0.0364 ]\n",
      "[epoch: 134] [loss:  0.0942 ] val_loss: [val_loss  0.0376 ]\n",
      "[epoch: 135] [loss:  0.0891 ] val_loss: [val_loss  0.0253 ]\n",
      "[epoch: 136] [loss:  0.0821 ] val_loss: [val_loss  0.0268 ]\n",
      "[epoch: 137] [loss:  0.0941 ] val_loss: [val_loss  0.0407 ]\n",
      "[epoch: 138] [loss:  0.1012 ] val_loss: [val_loss  0.0412 ]\n",
      "[epoch: 139] [loss:  0.1438 ] val_loss: [val_loss  0.0547 ]\n",
      "[epoch: 140] [loss:  0.1080 ] val_loss: [val_loss  0.0563 ]\n",
      "[epoch: 141] [loss:  0.0959 ] val_loss: [val_loss  0.0876 ]\n",
      "[epoch: 142] [loss:  0.0819 ] val_loss: [val_loss  0.0313 ]\n",
      "[epoch: 143] [loss:  0.0815 ] val_loss: [val_loss  0.0184 ]\n",
      "[epoch: 144] [loss:  0.0709 ] val_loss: [val_loss  0.0285 ]\n",
      "[epoch: 145] [loss:  0.0664 ] val_loss: [val_loss  0.0158 ]\n",
      "[epoch: 146] [loss:  0.0635 ] val_loss: [val_loss  0.0169 ]\n",
      "[epoch: 147] [loss:  0.0575 ] val_loss: [val_loss  0.0149 ]\n",
      "[epoch: 148] [loss:  0.0723 ] val_loss: [val_loss  0.0194 ]\n",
      "[epoch: 149] [loss:  0.0575 ] val_loss: [val_loss  0.0137 ]\n",
      "[epoch: 150] [loss:  0.0594 ] val_loss: [val_loss  0.0203 ]\n",
      "[epoch: 151] [loss:  0.0602 ] val_loss: [val_loss  0.0155 ]\n",
      "[epoch: 152] [loss:  0.0598 ] val_loss: [val_loss  0.0176 ]\n",
      "[epoch: 153] [loss:  0.0651 ] val_loss: [val_loss  0.0180 ]\n",
      "[epoch: 154] [loss:  0.0646 ] val_loss: [val_loss  0.0171 ]\n",
      "[epoch: 155] [loss:  0.0689 ] val_loss: [val_loss  0.0532 ]\n",
      "[epoch: 156] [loss:  0.1024 ] val_loss: [val_loss  0.0267 ]\n",
      "[epoch: 157] [loss:  0.0882 ] val_loss: [val_loss  0.0212 ]\n",
      "[epoch: 158] [loss:  0.0771 ] val_loss: [val_loss  0.0703 ]\n",
      "[epoch: 159] [loss:  0.0654 ] val_loss: [val_loss  0.0229 ]\n",
      "[epoch: 160] [loss:  0.0529 ] val_loss: [val_loss  0.0172 ]\n",
      "[epoch: 161] [loss:  0.0452 ] val_loss: [val_loss  0.0167 ]\n",
      "[epoch: 162] [loss:  0.0495 ] val_loss: [val_loss  0.0200 ]\n",
      "[epoch: 163] [loss:  0.0784 ] val_loss: [val_loss  0.0285 ]\n",
      "[epoch: 164] [loss:  0.1066 ] val_loss: [val_loss  0.0450 ]\n",
      "[epoch: 165] [loss:  0.1391 ] val_loss: [val_loss  0.0695 ]\n",
      "[epoch: 166] [loss:  0.1591 ] val_loss: [val_loss  0.2147 ]\n",
      "[epoch: 167] [loss:  0.2041 ] val_loss: [val_loss  0.0606 ]\n",
      "[epoch: 168] [loss:  0.2190 ] val_loss: [val_loss  0.0712 ]\n",
      "[epoch: 169] [loss:  0.1837 ] val_loss: [val_loss  0.0653 ]\n",
      "[epoch: 170] [loss:  0.1252 ] val_loss: [val_loss  0.0347 ]\n",
      "[epoch: 171] [loss:  0.1170 ] val_loss: [val_loss  0.0334 ]\n",
      "[epoch: 172] [loss:  0.0916 ] val_loss: [val_loss  0.0262 ]\n",
      "[epoch: 173] [loss:  0.0707 ] val_loss: [val_loss  0.0180 ]\n",
      "[epoch: 174] [loss:  0.0682 ] val_loss: [val_loss  0.0192 ]\n",
      "[epoch: 175] [loss:  0.0663 ] val_loss: [val_loss  0.0131 ]\n",
      "[epoch: 176] [loss:  0.0537 ] val_loss: [val_loss  0.0127 ]\n",
      "[epoch: 177] [loss:  0.0567 ] val_loss: [val_loss  0.0119 ]\n",
      "[epoch: 178] [loss:  0.0383 ] val_loss: [val_loss  0.0086 ]\n",
      "[epoch: 179] [loss:  0.0359 ] val_loss: [val_loss  0.0138 ]\n",
      "[epoch: 180] [loss:  0.0373 ] val_loss: [val_loss  0.0103 ]\n",
      "[epoch: 181] [loss:  0.0373 ] val_loss: [val_loss  0.0166 ]\n",
      "[epoch: 182] [loss:  0.0349 ] val_loss: [val_loss  0.0120 ]\n",
      "[epoch: 183] [loss:  0.0360 ] val_loss: [val_loss  0.0099 ]\n",
      "[epoch: 184] [loss:  0.0385 ] val_loss: [val_loss  0.0103 ]\n",
      "[epoch: 185] [loss:  0.0447 ] val_loss: [val_loss  0.0171 ]\n",
      "[epoch: 186] [loss:  0.0396 ] val_loss: [val_loss  0.0121 ]\n",
      "[epoch: 187] [loss:  0.0451 ] val_loss: [val_loss  0.0112 ]\n",
      "[epoch: 188] [loss:  0.0381 ] val_loss: [val_loss  0.0137 ]\n",
      "[epoch: 189] [loss:  0.0411 ] val_loss: [val_loss  0.0119 ]\n",
      "[epoch: 190] [loss:  0.0466 ] val_loss: [val_loss  0.0122 ]\n",
      "[epoch: 191] [loss:  0.0416 ] val_loss: [val_loss  0.0140 ]\n",
      "[epoch: 192] [loss:  0.0376 ] val_loss: [val_loss  0.0114 ]\n",
      "[epoch: 193] [loss:  0.0375 ] val_loss: [val_loss  0.0101 ]\n",
      "[epoch: 194] [loss:  0.0361 ] val_loss: [val_loss  0.0112 ]\n",
      "[epoch: 195] [loss:  0.0361 ] val_loss: [val_loss  0.0141 ]\n",
      "[epoch: 196] [loss:  0.0309 ] val_loss: [val_loss  0.0185 ]\n",
      "[epoch: 197] [loss:  0.0298 ] val_loss: [val_loss  0.0215 ]\n",
      "[epoch: 198] [loss:  0.0338 ] val_loss: [val_loss  0.0128 ]\n",
      "[epoch: 199] [loss:  0.0333 ] val_loss: [val_loss  0.0159 ]\n",
      "[epoch: 200] [loss:  0.0353 ] val_loss: [val_loss  0.0100 ]\n"
     ]
    }
   ],
   "source": [
    "#Training the model with complete data\n",
    "trained_model = train_transformer(\n",
    "    model,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    loss_func,\n",
    "    num_epochs,\n",
    "    lr = lr,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    warmup_interval=warmup_interval,\n",
    "    device=DEVICE\n",
    ")\n",
    "weights_path = os.path.join(GOOGLE_DRIVE_PATH, \"transformer.pt\")\n",
    "torch.save(trained_model.state_dict(), weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345b9bae",
   "metadata": {
    "id": "345b9bae"
   },
   "source": [
    "Run the cell below to get the accuracy on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0ad70d5b",
   "metadata": {
    "id": "0ad70d5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model accuracy:  0.9971\n"
     ]
    }
   ],
   "source": [
    "#Final validation accuracy\n",
    "print(\n",
    "    \"Final Model accuracy: \",\n",
    "    \"{:.4f}\".format(\n",
    "        val_transformer(\n",
    "            trained_model, valid_loader, LabelSmoothingLoss, 4, device=DEVICE\n",
    "        )[1]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19615d0-9b82-42ac-ba28-e079ca96aed2",
   "metadata": {
    "id": "c19615d0-9b82-42ac-ba28-e079ca96aed2"
   },
   "source": [
    "## Visualize and Inference: Model in action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f413c17-ede5-4d4d-a1fa-54458ca948c5",
   "metadata": {
    "id": "3f413c17-ede5-4d4d-a1fa-54458ca948c5"
   },
   "source": [
    "Now that we have trained a model, lets look at the final results. We will first look at the results from the validation data and visualize the attention weights (remember the self.weights_softmax?). These attention weights should give you some intuition about what the network learns. We have implemented everything for you here and the intention is to help you probe the model and understand about what does the network learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c39117ef-da8b-4fb0-904b-c5c4c37fc800",
   "metadata": {
    "id": "c39117ef-da8b-4fb0-904b-c5c4c37fc800"
   },
   "outputs": [],
   "source": [
    "import seaborn\n",
    "from a5_helper import inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137296b8-8ab8-4f9d-bff5-e2584370a757",
   "metadata": {
    "id": "137296b8-8ab8-4f9d-bff5-e2584370a757"
   },
   "source": [
    "### Results from the validation data\n",
    "\n",
    "In the below cell we pick the very first data point in the validation data and find the result on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "37c01cc4-96b0-4a4e-a820-64e5c2dd5549",
   "metadata": {
    "id": "37c01cc4-96b0-4a4e-a820-64e5c2dd5549"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence: \n",
      " BOS POSITIVE 47 add NEGATIVE 27 EOS\n"
     ]
    }
   ],
   "source": [
    "for it in valid_loader:\n",
    "    it\n",
    "    break\n",
    "inp, inp_pos, out, out_pos = it\n",
    "opposite_tokens_to_str = {v: k for k, v in convert_str_to_tokens.items()}\n",
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)\n",
    "inp_pos = inp_pos.to(device)\n",
    "out_pos = out_pos.to(device)\n",
    "out = out.to(device)\n",
    "inp = inp.to(device)\n",
    "\n",
    "inp_exp = inp[:1, :]\n",
    "inp_exp_pos = inp_pos[:1]\n",
    "out_pos_exp = out_pos[:1, :]\n",
    "inp_seq = [opposite_tokens_to_str[w.item()] for w in inp_exp[0]]\n",
    "print(\n",
    "    \"Input sequence: \\n\",\n",
    "    inp_seq[0]\n",
    "    + \" \"\n",
    "    + inp_seq[1]\n",
    "    + \" \"\n",
    "    + inp_seq[2]\n",
    "    + inp_seq[3]\n",
    "    + \" \"\n",
    "    + inp_seq[4]\n",
    "    + \" \"\n",
    "    + inp_seq[5]\n",
    "    + \" \"\n",
    "    + inp_seq[6]\n",
    "    + inp_seq[7]\n",
    "    + \" \"\n",
    "    + inp_seq[8],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ab535f1d-3be0-40ef-a0fd-8fca2fae2414",
   "metadata": {
    "id": "ab535f1d-3be0-40ef-a0fd-8fca2fae2414"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Sequence:\tBOS POSITIVE 2 0 \n"
     ]
    }
   ],
   "source": [
    "out_seq_ans, _ = inference(\n",
    "    trained_model, inp_exp, inp_exp_pos, out_pos_exp, out_seq_len\n",
    ")\n",
    "\n",
    "trained_model.eval()\n",
    "\n",
    "print(\"Output Sequence:\", end=\"\\t\")\n",
    "res = \"BOS \"\n",
    "for i in range(1, out_seq_ans.size(1)):\n",
    "    sym = opposite_tokens_to_str[out_seq_ans[0, i].item()]\n",
    "    if sym == \"EOS\":\n",
    "        break\n",
    "    res += sym + \" \"\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3412c073-d239-450a-aa46-9ec3d61309a6",
   "metadata": {
    "id": "3412c073-d239-450a-aa46-9ec3d61309a6"
   },
   "source": [
    "### Pick your own proboing example\n",
    "\n",
    "In the cell below, you could feed in an example in the input style, changing the variable `custom_seq`. We have filled a placeholder expression for you, but feel free to change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d7d2f3b1-ba94-4ccf-84e3-f3238059f25d",
   "metadata": {
    "id": "d7d2f3b1-ba94-4ccf-84e3-f3238059f25d"
   },
   "outputs": [],
   "source": [
    "custom_seq = \"BOS POSITIVE 20 subtract NEGATIVE 11 EOS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3caa768d-f61f-4ee4-8e87-259043e93cdb",
   "metadata": {
    "id": "3caa768d-f61f-4ee4-8e87-259043e93cdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Sequence:\tBOS POSITIVE 3 1 \n"
     ]
    }
   ],
   "source": [
    "out = prepocess_input_sequence(custom_seq, convert_str_to_tokens, SPECIAL_TOKENS)\n",
    "inp_exp = torch.tensor(out).to(DEVICE)\n",
    "\n",
    "out_seq_ans, model_for_visv = inference(\n",
    "    trained_model, inp_exp, inp_exp_pos, out_pos_exp, out_seq_len\n",
    ")\n",
    "\n",
    "trained_model.eval()\n",
    "\n",
    "print(\"Output Sequence:\", end=\"\\t\")\n",
    "res = \"BOS \"\n",
    "for i in range(1, out_seq_ans.size(1)):\n",
    "    sym = opposite_tokens_to_str[out_seq_ans[0, i].item()]\n",
    "    if sym == \"EOS\":\n",
    "        break\n",
    "    res += sym + \" \"\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9ae8cd-2813-4845-a4df-d47ebdc60971",
   "metadata": {
    "id": "0b9ae8cd-2813-4845-a4df-d47ebdc60971"
   },
   "source": [
    "### Visualize the attention weights\n",
    "\n",
    "In this part we will visualize the attention weights for the specific custom input you fed as input. There are seperate heatmaps for encoder and the decoder. The ligher value in color shows higher associated between the token present in that row and column, and darker color shows a weak relation between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8e82ff56-d55b-45b7-8507-514ee5968ed5",
   "metadata": {
    "id": "8e82ff56-d55b-45b7-8507-514ee5968ed5"
   },
   "outputs": [],
   "source": [
    "from a5_helper import draw\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "db22789f",
   "metadata": {
    "id": "db22789f"
   },
   "outputs": [],
   "source": [
    "target_exp = res.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9cae7e26-2587-4297-a44c-c59ab541dfc8",
   "metadata": {
    "id": "9cae7e26-2587-4297-a44c-c59ab541dfc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Block Number 1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'EncoderBlock' object has no attribute 'MultiHeadBlock'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/shao-yu-huang/Desktop/ml-workspace/EECS498_2022/A5/Transformers.ipynb Cell 112\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bubuntu/home/shao-yu-huang/Desktop/ml-workspace/EECS498_2022/A5/Transformers.ipynb#Y216sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEncoder Block Number\u001b[39m\u001b[39m\"\u001b[39m, layer \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bubuntu/home/shao-yu-huang/Desktop/ml-workspace/EECS498_2022/A5/Transformers.ipynb#Y216sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_heads):\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bubuntu/home/shao-yu-huang/Desktop/ml-workspace/EECS498_2022/A5/Transformers.ipynb#Y216sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         draw(\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bubuntu/home/shao-yu-huang/Desktop/ml-workspace/EECS498_2022/A5/Transformers.ipynb#Y216sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m             trained_model\u001b[39m.\u001b[39;49mencoder\u001b[39m.\u001b[39;49mlayers[layer]\n\u001b[0;32m----> <a href='vscode-notebook-cell://tunnel%2Bubuntu/home/shao-yu-huang/Desktop/ml-workspace/EECS498_2022/A5/Transformers.ipynb#Y216sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m             \u001b[39m.\u001b[39;49mMultiHeadBlock\u001b[39m.\u001b[39mheads[h]\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bubuntu/home/shao-yu-huang/Desktop/ml-workspace/EECS498_2022/A5/Transformers.ipynb#Y216sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m             \u001b[39m.\u001b[39mweights_softmax\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bubuntu/home/shao-yu-huang/Desktop/ml-workspace/EECS498_2022/A5/Transformers.ipynb#Y216sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m             \u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bubuntu/home/shao-yu-huang/Desktop/ml-workspace/EECS498_2022/A5/Transformers.ipynb#Y216sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m             inp_seq,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bubuntu/home/shao-yu-huang/Desktop/ml-workspace/EECS498_2022/A5/Transformers.ipynb#Y216sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m             inp_seq \u001b[39mif\u001b[39;00m h \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m [],\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bubuntu/home/shao-yu-huang/Desktop/ml-workspace/EECS498_2022/A5/Transformers.ipynb#Y216sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m             ax\u001b[39m=\u001b[39maxs[h],\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bubuntu/home/shao-yu-huang/Desktop/ml-workspace/EECS498_2022/A5/Transformers.ipynb#Y216sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bubuntu/home/shao-yu-huang/Desktop/ml-workspace/EECS498_2022/A5/Transformers.ipynb#Y216sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bubuntu/home/shao-yu-huang/Desktop/ml-workspace/EECS498_2022/A5/Transformers.ipynb#Y216sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m plt\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EncoderBlock' object has no attribute 'MultiHeadBlock'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for layer in range(num_enc_layers):\n",
    "    fig, axs = plt.subplots(1, num_heads, figsize=(20, 10))\n",
    "    print(\"Encoder Block Number\", layer + 1)\n",
    "    for h in range(num_heads):\n",
    "        draw(\n",
    "            trained_model.encoder.layers[layer]\n",
    "            .MultiHeadBlock.heads[h]\n",
    "            .weights_softmax.data.cpu()\n",
    "            .numpy()[0],\n",
    "            inp_seq,\n",
    "            inp_seq if h == 0 else [],\n",
    "            ax=axs[h],\n",
    "        )\n",
    "    plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "be6ba6cb",
   "metadata": {
    "id": "be6ba6cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Block number  1\n",
      "Decoder Self Attention 1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MultiHeadAttention' object has no attribute 'heads'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/shao-yu-huang/Desktop/ml-workspace/EECS498_2022/A5/Transformers.ipynb Cell 113\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bubuntu/home/shao-yu-huang/Desktop/ml-workspace/EECS498_2022/A5/Transformers.ipynb#Y220sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDecoder Self Attention\u001b[39m\u001b[39m\"\u001b[39m, layer \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bubuntu/home/shao-yu-huang/Desktop/ml-workspace/EECS498_2022/A5/Transformers.ipynb#Y220sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_heads):\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bubuntu/home/shao-yu-huang/Desktop/ml-workspace/EECS498_2022/A5/Transformers.ipynb#Y220sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     draw(\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bubuntu/home/shao-yu-huang/Desktop/ml-workspace/EECS498_2022/A5/Transformers.ipynb#Y220sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         trained_model\u001b[39m.\u001b[39;49mdecoder\u001b[39m.\u001b[39;49mlayers[layer]\n\u001b[0;32m---> <a href='vscode-notebook-cell://tunnel%2Bubuntu/home/shao-yu-huang/Desktop/ml-workspace/EECS498_2022/A5/Transformers.ipynb#Y220sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         \u001b[39m.\u001b[39;49mattention_self\u001b[39m.\u001b[39;49mheads[h]\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bubuntu/home/shao-yu-huang/Desktop/ml-workspace/EECS498_2022/A5/Transformers.ipynb#Y220sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39m.\u001b[39mweights_softmax\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bubuntu/home/shao-yu-huang/Desktop/ml-workspace/EECS498_2022/A5/Transformers.ipynb#Y220sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bubuntu/home/shao-yu-huang/Desktop/ml-workspace/EECS498_2022/A5/Transformers.ipynb#Y220sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m         target_exp,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bubuntu/home/shao-yu-huang/Desktop/ml-workspace/EECS498_2022/A5/Transformers.ipynb#Y220sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m         target_exp \u001b[39mif\u001b[39;00m h \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m [],\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bubuntu/home/shao-yu-huang/Desktop/ml-workspace/EECS498_2022/A5/Transformers.ipynb#Y220sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m         ax\u001b[39m=\u001b[39maxs[h],\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bubuntu/home/shao-yu-huang/Desktop/ml-workspace/EECS498_2022/A5/Transformers.ipynb#Y220sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bubuntu/home/shao-yu-huang/Desktop/ml-workspace/EECS498_2022/A5/Transformers.ipynb#Y220sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m plt\u001b[39m.\u001b[39mshow()\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bubuntu/home/shao-yu-huang/Desktop/ml-workspace/EECS498_2022/A5/Transformers.ipynb#Y220sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDecoder Cross attention\u001b[39m\u001b[39m\"\u001b[39m, layer \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MultiHeadAttention' object has no attribute 'heads'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for layer in range(num_dec_layers):\n",
    "    fig, axs = plt.subplots(1, num_heads, figsize=(20, 10))\n",
    "\n",
    "    print(\"Decoder Block number \", layer + 1)\n",
    "\n",
    "    print(\"Decoder Self Attention\", layer + 1)\n",
    "    for h in range(num_heads):\n",
    "        draw(\n",
    "            trained_model.decoder.layers[layer]\n",
    "            .attention_self.heads[h]\n",
    "            .weights_softmax.data.cpu()\n",
    "            .numpy()[0],\n",
    "            target_exp,\n",
    "            target_exp if h == 0 else [],\n",
    "            ax=axs[h],\n",
    "        )\n",
    "    plt.show()\n",
    "    print(\"Decoder Cross attention\", layer + 1)\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "    for h in range(num_heads):\n",
    "        draw(\n",
    "            trained_model.decoder.layers[layer]\n",
    "            .attention_cross.heads[h]\n",
    "            .weights_softmax.data.cpu()\n",
    "            .numpy()[0],\n",
    "            inp_seq,\n",
    "            target_exp if h == 0 else [],\n",
    "            ax=axs[h],\n",
    "        )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a52caf-c12e-4ca9-986b-cc52fe50545a",
   "metadata": {
    "id": "c8a52caf-c12e-4ca9-986b-cc52fe50545a"
   },
   "source": [
    "# Submit Your Work\n",
    "After completing both notebooks for this assignment (`transformers.ipynb` and this notebook, `rnn_lstm_captionaing.ipynb`), run the following cell to create a `.zip` file for you to download and turn in. \n",
    "\n",
    "**Please MANUALLY SAVE every `*.ipynb` and `*.py` files before executing the following cell:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82833983",
   "metadata": {
    "id": "82833983"
   },
   "outputs": [],
   "source": [
    "from eecs598.submit import make_a5_submission\n",
    "\n",
    "# TODO: Replace these with your actual uniquename and umid\n",
    "uniquename = None\n",
    "umid = None\n",
    "make_a5_submission(GOOGLE_DRIVE_PATH, uniquename, umid)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "b43b5e66-7d96-49a7-8d73-649c1d8de2ef",
    "137296b8-8ab8-4f9d-bff5-e2584370a757",
    "3412c073-d239-450a-aa46-9ec3d61309a6",
    "0b9ae8cd-2813-4845-a4df-d47ebdc60971"
   ],
   "name": "Transformers.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "3e6a8e772529b48ea93620fbc55d49ea9e469a86dedbc53ac24607b5264d00e5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
